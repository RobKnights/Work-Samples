# -*- coding: utf-8 -*-
"""db-final-spark-ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DmvzhAaPNBBtGPHpBOyBvFZ2aflWMf6e
"""

!pip install pyarrow

!pip install s3fs

import duckdb
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

print("‚úì Packages installed successfully!")

# Create DuckDB connection
con = duckdb.connect(database=':memory:', read_only=False)

# Install and load httpfs extension for S3 access
con.execute("INSTALL httpfs;")
con.execute("LOAD httpfs;")

# Configure S3 access (anonymous)
con.execute("SET s3_region='us-east-1';")
con.execute("SET s3_access_key_id='';")
con.execute("SET s3_secret_access_key='';")
con.execute("SET s3_endpoint='s3.amazonaws.com';")

print("‚úì DuckDB configured for S3 access")

print("\n" + "="*60)
print("EXPLORING DATA STRUCTURE")
print("="*60)

# First, let's see what columns are available
comments_path = "s3://qg65-dsan6000/reddit/comments/*.parquet"
submissions_path = "s3://qg65-dsan6000/reddit/submissions/*.parquet"

# Get schema for comments
print("\nComments Schema:")
comments_schema = con.execute(f"""
    DESCRIBE SELECT * FROM read_parquet('{comments_path}') LIMIT 1
""").fetchdf()
print(comments_schema)

print("\nSubmissions Schema:")
submissions_schema = con.execute(f"""
    DESCRIBE SELECT * FROM read_parquet('{submissions_path}') LIMIT 1
""").fetchdf()
print(submissions_schema)

# Get sample data
print("\n" + "-"*60)
print("SAMPLE COMMENTS DATA")
print("-"*60)
sample_comments = con.execute(f"""
    SELECT * FROM read_parquet('{comments_path}') LIMIT 5
""").fetchdf()
print(sample_comments)

print("\n" + "-"*60)
print("SAMPLE SUBMISSIONS DATA")
print("-"*60)
sample_submissions = con.execute(f"""
    SELECT * FROM read_parquet('{submissions_path}') LIMIT 5
""").fetchdf()
print(sample_submissions)













print("\n" + "="*60)
print("DATA OVERVIEW")
print("="*60)

# Count total records
print("\nCounting records (this may take a minute)...")

comments_count = con.execute(f"""
    SELECT COUNT(*) as count FROM read_parquet('{comments_path}')
""").fetchdf()['count'][0]

submissions_count = con.execute(f"""
    SELECT COUNT(*) as count FROM read_parquet('{submissions_path}')
""").fetchdf()['count'][0]

print(f"‚úì Total comments: {comments_count:,}")
print(f"‚úì Total submissions: {submissions_count:,}")

# Find top subreddits
print("\n" + "-"*60)
print("TOP 30 SUBREDDITS IN COMMENTS")
print("-"*60)

top_comment_subs = con.execute(f"""
    SELECT
        subreddit,
        COUNT(*) as comment_count,
        COUNT(DISTINCT author) as unique_authors
    FROM read_parquet('{comments_path}')
    WHERE author NOT IN ('[deleted]', '[removed]', 'AutoModerator')
        AND author IS NOT NULL
    GROUP BY subreddit
    ORDER BY comment_count DESC
    LIMIT 30
""").fetchdf()
print(top_comment_subs)

print("\n" + "-"*60)
print("TOP 30 SUBREDDITS IN SUBMISSIONS")
print("-"*60)

top_submission_subs = con.execute(f"""
    SELECT
        subreddit,
        COUNT(*) as submission_count,
        COUNT(DISTINCT author) as unique_authors
    FROM read_parquet('{submissions_path}')
    WHERE author NOT IN ('[deleted]', '[removed]', 'AutoModerator')
        AND author IS NOT NULL
    GROUP BY subreddit
    ORDER BY submission_count DESC
    LIMIT 30
""").fetchdf()
print(top_submission_subs)

"""## FILTERING FOR CONSUMER SUBREDDITS


"""

print("\n" + "="*60)
print("FILTERING FOR CONSUMER SUBREDDITS")
print("="*60)

# Based on your data, let's define consumer-related subreddits
# These include technology, products, shopping, and consumer advice subreddits
consumer_subreddits = [
    # Technology & Electronics
    'buildapc', 'pcmasterrace', 'technology', 'iphone', 'android', 'smartphones',
    'laptops', 'gadgets', 'homeautomation', 'hometheater', 'mechanicalkeyboards',

    #finance and investments
    'stock', 'realestate', 'investing', 'forex', 'crypto','cryptocurrency', 'business',
    'housing', 'insurance', 'education', 'tuition', 'healthcare', 'investmentadvice'

    # Fashion & Lifestyle
    'watches', 'sneakers', 'fashion', 'malefashionadvice', 'femalefashionadvice',
    'streetwear', 'makeupaddiction', 'skincareaddiction', 'cosmetics', 'luxury', 'bag'

    # Shopping & Deals
    'frugal', 'buyitforlife', 'deals', 'shopping', 'amazondeals', 'coupons',
    'anticonsumption', 'consumerism', 'discounts'

    # Home & Living
    'homeimprovement', 'furniture', 'interiordesign', 'appliances',

    # Gaming (consumer products)
    'gaming', 'videogames', 'minecraft', 'overwatch', 'leagueoflegends',
    'genshin_impact', 'valorant', 'steam', 'switch',

    # Hobbies & Products
    'cooking', 'motorcycles', 'cars', 'travel', 'music', 'pop','concerts', 'album',
    'sports', 'tennis','basketball', 'football',
    'soccer', 'nba', 'movies', 'books', 'foods'
]

# Create SQL list for filtering
consumer_subs_sql = ", ".join([f"'{sub.lower()}'" for sub in consumer_subreddits])

# Check which consumer subreddits exist in our data
print("\nChecking which consumer subreddits exist in the data...")
existing_subs = con.execute(f"""
    SELECT subreddit,
           SUM(comment_count) as total_comments,
           SUM(unique_authors) as total_authors
    FROM (
        SELECT LOWER(subreddit) as subreddit,
               COUNT(*) as comment_count,
               COUNT(DISTINCT author) as unique_authors
        FROM read_parquet('{comments_path}')
        WHERE LOWER(subreddit) IN ({consumer_subs_sql})
            AND author NOT IN ('[deleted]', '[removed]', 'AutoModerator')
            AND author IS NOT NULL
        GROUP BY LOWER(subreddit)

        UNION ALL

        SELECT LOWER(subreddit) as subreddit,
               0 as comment_count,
               COUNT(DISTINCT author) as unique_authors
        FROM read_parquet('{submissions_path}')
        WHERE LOWER(subreddit) IN ({consumer_subs_sql})
            AND author NOT IN ('[deleted]', '[removed]', 'AutoModerator')
            AND author IS NOT NULL
        GROUP BY LOWER(subreddit)
    )
    GROUP BY subreddit
    ORDER BY total_comments DESC
""").fetchdf()

print(f"‚úì Found {len(existing_subs)} consumer subreddits in data:")
print(existing_subs)

if len(existing_subs) > 0:
    # Use only subreddits that exist in the data
    consumer_subs_sql = ", ".join([f"'{sub}'" for sub in existing_subs['subreddit'].tolist()])
    print(f"\n‚úì Using {len(existing_subs)} consumer subreddits for analysis")
else:
    print("\n‚ö† WARNING: None of the predefined consumer subreddits found in data.")
    print("This shouldn't happen based on your output. Using top subreddits as fallback...")

    # Fallback: use most active subreddits
    top_subs = con.execute(f"""
        SELECT LOWER(subreddit) as subreddit, COUNT(*) as count
        FROM (
            SELECT subreddit FROM read_parquet('{comments_path}')
            UNION ALL
            SELECT subreddit FROM read_parquet('{submissions_path}')
        )
        GROUP BY LOWER(subreddit)
        ORDER BY count DESC
        LIMIT 20
    """).fetchdf()

    consumer_subs_sql = ", ".join([f"'{sub}'" for sub in top_subs['subreddit'].tolist()])
    print(f"Using top {len(top_subs)} subreddits instead")

print("\n" + "="*60)
print("CREATING USER RETENTION DATASET")
print("="*60)

# Create a combined view of all user activities with better performance
print("\nCombining comments and submissions (this may take several minutes)...")

user_retention_query = f"""
WITH all_activities AS (
    -- Comments
    SELECT
        author as user,
        LOWER(subreddit) as subreddit,
        created_utc as timestamp,
        'comment' as activity_type,
        score
    FROM read_parquet('{comments_path}')
    WHERE author NOT IN ('[deleted]', '[removed]', 'AutoModerator')
        AND author IS NOT NULL
        AND LOWER(subreddit) IN ({consumer_subs_sql})

    UNION ALL

    -- Submissions
    SELECT
        author as user,
        LOWER(subreddit) as subreddit,
        created_utc as timestamp,
        'submission' as activity_type,
        score
    FROM read_parquet('{submissions_path}')
    WHERE author NOT IN ('[deleted]', '[removed]', 'AutoModerator')
        AND author IS NOT NULL
        AND LOWER(subreddit) IN ({consumer_subs_sql})
),

user_metrics AS (
    SELECT
        user,

        -- Time-based metrics
        MIN(timestamp) as first_activity_timestamp,
        MAX(timestamp) as last_activity_timestamp,
        (MAX(timestamp) - MIN(timestamp)) / 86400.0 as retention_days,

        -- Activity counts
        COUNT(*) as total_activities,
        COUNT(DISTINCT subreddit) as unique_subreddits,
        SUM(CASE WHEN activity_type = 'comment' THEN 1 ELSE 0 END) as num_comments,
        SUM(CASE WHEN activity_type = 'submission' THEN 1 ELSE 0 END) as num_submissions,

        -- Engagement metrics
        AVG(score) as avg_score,
        MAX(score) as max_score,
        SUM(score) as total_score,

        -- Time span analysis
        COUNT(DISTINCT DATE_TRUNC('day', to_timestamp(timestamp))) as active_days,
        COUNT(DISTINCT DATE_TRUNC('week', to_timestamp(timestamp))) as active_weeks,
        COUNT(DISTINCT DATE_TRUNC('month', to_timestamp(timestamp))) as active_months,

        -- Dates
        to_timestamp(MIN(timestamp))::DATE as first_activity_date,
        to_timestamp(MAX(timestamp))::DATE as last_activity_date,

        -- Recency
        (EXTRACT(EPOCH FROM CURRENT_TIMESTAMP) - MAX(timestamp)) / 86400.0 as days_since_last_activity,
        (EXTRACT(EPOCH FROM CURRENT_TIMESTAMP) - MIN(timestamp)) / 86400.0 as account_age_days

    FROM all_activities
    GROUP BY user
    HAVING COUNT(*) >= 2  -- Only users with at least 2 activities
)

SELECT
    user,
    first_activity_timestamp,
    last_activity_timestamp,
    first_activity_date,
    last_activity_date,
    retention_days,
    total_activities,
    unique_subreddits,
    num_comments,
    num_submissions,
    active_days,
    active_weeks,
    active_months,
    avg_score,
    max_score,
    total_score,
    days_since_last_activity,
    account_age_days,

    -- Derived features
    ROUND(total_activities::FLOAT / (retention_days + 1), 4) as activity_frequency,
    ROUND(total_activities::FLOAT / NULLIF(active_days, 0), 2) as activities_per_active_day,
    ROUND(active_days::FLOAT / NULLIF(active_weeks, 0), 2) as avg_active_days_per_week,

    CASE
        WHEN num_submissions > 0 THEN ROUND(num_comments::FLOAT / num_submissions, 2)
        ELSE num_comments::FLOAT
    END as comment_submission_ratio,

    -- User type based on activity pattern
    CASE
        WHEN num_comments > num_submissions * 5 THEN 'commenter'
        WHEN num_submissions > num_comments THEN 'submitter'
        ELSE 'balanced'
    END as user_type,

    -- Engagement level category (based on total activities)
    CASE
        WHEN total_activities < 5 THEN 'low'
        WHEN total_activities < 20 THEN 'medium'
        WHEN total_activities < 100 THEN 'high'
        ELSE 'very_high'
    END as engagement_level,

    -- Consistency (how regularly they post)
    CASE
        WHEN active_days::FLOAT / NULLIF(retention_days + 1, 0) > 0.5 THEN 'very_consistent'
        WHEN active_days::FLOAT / NULLIF(retention_days + 1, 0) > 0.2 THEN 'consistent'
        WHEN active_days::FLOAT / NULLIF(retention_days + 1, 0) > 0.05 THEN 'sporadic'
        ELSE 'rare'
    END as consistency,

    -- Retention category (TARGET VARIABLE for ML)
    CASE
        WHEN retention_days < 7 THEN 'short'
        WHEN retention_days < 30 THEN 'medium'
        WHEN retention_days < 90 THEN 'long'
        ELSE 'very_long'
    END as retention_category,

    -- Recency category (for RFM-style analysis)
    CASE
        WHEN days_since_last_activity < 7 THEN 'active'
        WHEN days_since_last_activity < 30 THEN 'recent'
        WHEN days_since_last_activity < 90 THEN 'inactive'
        ELSE 'churned'
    END as recency_status

FROM user_metrics
ORDER BY retention_days DESC
"""

print("Executing query (this will take several minutes for large datasets)...")
print("‚è≥ Please wait...")

user_retention_df = con.execute(user_retention_query).fetchdf()

print(f"\n‚úì Created retention dataset with {len(user_retention_df):,} users")
print(f"‚úì Features: {len(user_retention_df.columns)} columns")
print(f"‚úì Memory usage: {user_retention_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

print("\n" + "="*60)
print("DETAILED RETENTION ANALYSIS")
print("="*60)

# 1. Summary statistics
print("\nüìä SUMMARY STATISTICS - KEY METRICS")
print("-"*60)
summary_stats = user_retention_df[['retention_days', 'total_activities', 'unique_subreddits',
                                     'activity_frequency', 'avg_score', 'active_days']].describe()
print(summary_stats.round(2))

# 2. Retention category distribution
print("\nüìà RETENTION CATEGORY DISTRIBUTION (TARGET VARIABLE)")
print("-"*60)
retention_dist = user_retention_df['retention_category'].value_counts().sort_index()
retention_pct = (retention_dist / len(user_retention_df) * 100).round(2)
retention_summary = pd.DataFrame({
    'Count': retention_dist,
    'Percentage': retention_pct
})
print(retention_summary)

# 3. Engagement level distribution
print("\nüë• ENGAGEMENT LEVEL DISTRIBUTION")
print("-"*60)
engagement_dist = user_retention_df['engagement_level'].value_counts().sort_index()
engagement_pct = (engagement_dist / len(user_retention_df) * 100).round(2)
engagement_summary = pd.DataFrame({
    'Count': engagement_dist,
    'Percentage': engagement_pct
})
print(engagement_summary)

# 4. User type distribution
print("\nüé≠ USER TYPE DISTRIBUTION")
print("-"*60)
user_type_dist = user_retention_df['user_type'].value_counts()
print(user_type_dist)

# 5. Consistency distribution
print("\n‚ö° POSTING CONSISTENCY DISTRIBUTION")
print("-"*60)
consistency_dist = user_retention_df['consistency'].value_counts().sort_index()
print(consistency_dist)

# 6. Recency status
print("\nüïí RECENCY STATUS (Current User State)")
print("-"*60)
recency_dist = user_retention_df['recency_status'].value_counts()
print(recency_dist)

# 7. Cross-tabulation: Retention by Engagement
print("\nüìä RETENTION CATEGORY BY ENGAGEMENT LEVEL")
print("-"*60)
cross_tab = pd.crosstab(user_retention_df['engagement_level'],
                        user_retention_df['retention_category'],
                        margins=True)
print(cross_tab)

# 8. Average retention by engagement level
print("\nüìà AVERAGE RETENTION DAYS BY ENGAGEMENT LEVEL")
print("-"*60)
avg_retention = user_retention_df.groupby('engagement_level')['retention_days'].agg(['mean', 'median', 'count'])
print(avg_retention.round(2))

# 9. Top performers
print("\nüèÜ TOP 20 USERS BY RETENTION PERIOD")
print("-"*60)
top_users = user_retention_df.nlargest(20, 'retention_days')[
    ['user', 'retention_days', 'total_activities', 'unique_subreddits',
     'activity_frequency', 'user_type', 'retention_category']
]
print(top_users.to_string(index=False))

# 10. Most engaged users (high activity in short time)
print("\n‚ö° MOST ENGAGED USERS (High Activity Frequency)")
print("-"*60)
most_engaged = user_retention_df.nlargest(20, 'activity_frequency')[
    ['user', 'retention_days', 'total_activities', 'activity_frequency',
     'activities_per_active_day']
]
print(most_engaged.to_string(index=False))

# 11. Potential churn users (high activity but short retention)
print("\n‚ö†Ô∏è POTENTIAL CHURN RISK (High Activity + Short Retention)")
print("-"*60)
churn_risk = user_retention_df[
    (user_retention_df['total_activities'] >= 10) &
    (user_retention_df['retention_days'] < 7)
].nlargest(10, 'total_activities')[
    ['user', 'retention_days', 'total_activities', 'activity_frequency', 'avg_score']
]
if len(churn_risk) > 0:
    print(churn_risk.to_string(index=False))
else:
    print("No users found matching criteria")

# 12. Data coverage
print("\nüìÖ DATA COVERAGE")
print("-"*60)
print(f"First activity in dataset: {user_retention_df['first_activity_date'].min()}")
print(f"Last activity in dataset: {user_retention_df['last_activity_date'].max()}")
data_span = (user_retention_df['last_activity_date'].max() -
             user_retention_df['first_activity_date'].min()).days
print(f"Total data span: {data_span} days ({data_span/30:.1f} months)")
print(f"\nAverage user account age: {user_retention_df['account_age_days'].mean():.1f} days")
print(f"Average days since last activity: {user_retention_df['days_since_last_activity'].mean():.1f} days")

# 13. Correlation analysis for key features
print("\nüîó CORRELATION WITH RETENTION DAYS")
print("-"*60)
correlations = user_retention_df[[
    'retention_days', 'total_activities', 'unique_subreddits',
    'activity_frequency', 'avg_score', 'active_days', 'active_weeks'
]].corr()['retention_days'].sort_values(ascending=False)
print(correlations.round(3))

print("\n" + "="*60)
print("CREATING ENHANCED VISUALIZATIONS")
print("="*60)

sns.set_style('whitegrid')
fig = plt.figure(figsize=(20, 12))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# 1. Retention days distribution (log scale)
ax1 = fig.add_subplot(gs[0, 0])
ax1.hist(user_retention_df['retention_days'], bins=100, edgecolor='black', color='steelblue', alpha=0.7)
ax1.axvline(user_retention_df['retention_days'].median(), color='red',
            linestyle='--', linewidth=2, label=f'Median: {user_retention_df["retention_days"].median():.1f}d')
ax1.axvline(user_retention_df['retention_days'].mean(), color='orange',
            linestyle='--', linewidth=2, label=f'Mean: {user_retention_df["retention_days"].mean():.1f}d')
ax1.set_xlabel('Retention Days', fontsize=10)
ax1.set_ylabel('Number of Users', fontsize=10)
ax1.set_title('User Retention Distribution', fontsize=11, fontweight='bold')
ax1.legend()
ax1.set_yscale('log')

# 2. Retention category
ax2 = fig.add_subplot(gs[0, 1])
retention_counts = user_retention_df['retention_category'].value_counts().sort_index()
colors = ['#ff6b6b', '#ffd93d', '#6bcf7f', '#4d96ff']
bars = ax2.bar(range(len(retention_counts)), retention_counts.values,
               color=colors, edgecolor='black', alpha=0.8)
ax2.set_xticks(range(len(retention_counts)))
ax2.set_xticklabels(retention_counts.index, rotation=0)
ax2.set_xlabel('Retention Category', fontsize=10)
ax2.set_ylabel('Number of Users', fontsize=10)
ax2.set_title('Users by Retention Category', fontsize=11, fontweight='bold')
# Add percentage labels on bars
for i, (bar, count) in enumerate(zip(bars, retention_counts.values)):
    height = bar.get_height()
    pct = count / len(user_retention_df) * 100
    ax2.text(bar.get_x() + bar.get_width()/2., height,
             f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)

# 3. Engagement level
ax3 = fig.add_subplot(gs[0, 2])
engagement_counts = user_retention_df['engagement_level'].value_counts().sort_index()
ax3.bar(range(len(engagement_counts)), engagement_counts.values,
        color='coral', edgecolor='black', alpha=0.8)
ax3.set_xticks(range(len(engagement_counts)))
ax3.set_xticklabels(engagement_counts.index, rotation=0, fontsize=9)
ax3.set_xlabel('Engagement Level', fontsize=10)
ax3.set_ylabel('Number of Users', fontsize=10)
ax3.set_title('Users by Engagement Level', fontsize=11, fontweight='bold')

# 4. Activity frequency vs retention (scatter with density)
ax4 = fig.add_subplot(gs[1, 0])
sample = user_retention_df.sample(min(10000, len(user_retention_df)))
scatter = ax4.scatter(sample['activity_frequency'], sample['retention_days'],
                      alpha=0.4, s=10, c=sample['total_activities'],
                      cmap='viridis', edgecolors='none')
ax4.set_xlabel('Activity Frequency (activities/day)', fontsize=10)
ax4.set_ylabel('Retention Days', fontsize=10)
ax4.set_title('Activity Frequency vs Retention', fontsize=11, fontweight='bold')
ax4.set_xlim(0, user_retention_df['activity_frequency'].quantile(0.95))
ax4.set_ylim(0, user_retention_df['retention_days'].quantile(0.95))
plt.colorbar(scatter, ax=ax4, label='Total Activities')

# 5. Box plot: Retention by engagement level
ax5 = fig.add_subplot(gs[1, 1])
engagement_order = ['low', 'medium', 'high', 'very_high']
data_to_plot = [user_retention_df[user_retention_df['engagement_level'] == level]['retention_days'].values
                for level in engagement_order if level in user_retention_df['engagement_level'].unique()]
bp = ax5.boxplot(data_to_plot, labels=[l for l in engagement_order if l in user_retention_df['engagement_level'].unique()],
                 patch_artist=True, showfliers=False)
for patch, color in zip(bp['boxes'], ['#ff6b6b', '#ffd93d', '#6bcf7f', '#4d96ff']):
    patch.set_facecolor(color)
    patch.set_alpha(0.7)
ax5.set_xlabel('Engagement Level', fontsize=10)
ax5.set_ylabel('Retention Days', fontsize=10)
ax5.set_title('Retention Distribution by Engagement', fontsize=11, fontweight='bold')
ax5.tick_params(axis='x', rotation=0, labelsize=9)

# 6. User type distribution
ax6 = fig.add_subplot(gs[1, 2])
user_type_counts = user_retention_df['user_type'].value_counts()
ax6.pie(user_type_counts.values, labels=user_type_counts.index, autopct='%1.1f%%',
        colors=['#ff9999', '#66b3ff', '#99ff99'], startangle=90)
ax6.set_title('User Type Distribution', fontsize=11, fontweight='bold')

# 7. Active days vs retention
ax7 = fig.add_subplot(gs[2, 0])
sample = user_retention_df.sample(min(10000, len(user_retention_df)))
ax7.scatter(sample['active_days'], sample['retention_days'],
            alpha=0.3, s=10, color='purple')
ax7.set_xlabel('Active Days', fontsize=10)
ax7.set_ylabel('Retention Days', fontsize=10)
ax7.set_title('Active Days vs Retention Period', fontsize=11, fontweight='bold')
ax7.plot([0, sample['retention_days'].max()], [0, sample['retention_days'].max()],
         'r--', alpha=0.5, label='Perfect consistency')
ax7.legend(fontsize=8)

# 8. Recency status
ax8 = fig.add_subplot(gs[2, 1])
recency_counts = user_retention_df['recency_status'].value_counts()
colors_recency = {'active': '#2ecc71', 'recent': '#f39c12', 'inactive': '#e74c3c', 'churned': '#95a5a6'}
bars = ax8.bar(range(len(recency_counts)), recency_counts.values,
               color=[colors_recency.get(x, 'gray') for x in recency_counts.index],
               edgecolor='black', alpha=0.8)
ax8.set_xticks(range(len(recency_counts)))
ax8.set_xticklabels(recency_counts.index, rotation=45, ha='right', fontsize=9)
ax8.set_xlabel('Recency Status', fontsize=10)
ax8.set_ylabel('Number of Users', fontsize=10)
ax8.set_title('Current User Status', fontsize=11, fontweight='bold')

# 9. Consistency distribution
ax9 = fig.add_subplot(gs[2, 2])
consistency_counts = user_retention_df['consistency'].value_counts().sort_index()
ax9.barh(range(len(consistency_counts)), consistency_counts.values,
         color='teal', edgecolor='black', alpha=0.8)
ax9.set_yticks(range(len(consistency_counts)))
ax9.set_yticklabels(consistency_counts.index, fontsize=9)
ax9.set_xlabel('Number of Users', fontsize=10)
ax9.set_ylabel('Consistency Level', fontsize=10)
ax9.set_title('User Posting Consistency', fontsize=11, fontweight='bold')

plt.suptitle('Reddit User Retention Analysis - Comprehensive Overview',
             fontsize=16, fontweight='bold', y=0.995)
plt.savefig('retention_analysis_comprehensive.png', dpi=300, bbox_inches='tight')
print("‚úì Saved comprehensive visualization to 'retention_analysis_comprehensive.png'")
plt.show()

print("\n" + "="*60)
print("SAVING PROCESSED DATA & REPORTS")
print("="*60)

# 1. Save main dataset
user_retention_df.to_csv('reddit_user_retention_features.csv', index=False)
print("‚úì Saved to 'reddit_user_retention_features.csv'")

user_retention_df.to_parquet('reddit_user_retention_features.parquet', index=False)
print("‚úì Saved to 'reddit_user_retention_features.parquet'")

from google.colab import drive
drive.mount('/content/drive')

import os

output_dir = "/content/drive/My Drive/reddit_analysis/"
os.makedirs(output_dir, exist_ok=True)
output_dir

csv_path = output_dir + "reddit_user_retention_features.csv"
parquet_path = output_dir + "reddit_user_retention_features.parquet"


user_retention_df.to_csv(csv_path, index=False)
print(f"‚úì Saved CSV to {csv_path}")

user_retention_df.to_parquet(parquet_path, index=False)
print(f"‚úì Saved Parquet to {parquet_path}")

import shutil

src = "/content/reddit_retention_ml_ready.csv"
dst = output_dir + "reddit_retention_ml_ready.csv"

shutil.copy(src, dst)

print(f"‚úì Saved to {dst}")

"""‚úì Saved CSV to /content/drive/My Drive/reddit_analysis/reddit_user_retention_features.csv

‚úì Saved Parquet to /content/drive/My Drive/reddit_analysis/reddit_user_retention_features.parquet

## csv saved
"""

ml_ready_df.info()

# 2. Create ML-ready dataset (encoded categorical variables)
ml_ready_df = user_retention_df.copy()

# Convert timestamps to numeric features
ml_ready_df['first_activity_timestamp_normalized'] = (
    ml_ready_df['first_activity_timestamp'] - ml_ready_df['first_activity_timestamp'].min()
) / 86400

# One-hot encode categorical variables
categorical_cols = ['user_type', 'engagement_level', 'consistency', 'retention_category', 'recency_status']
ml_ready_df = pd.get_dummies(ml_ready_df, columns=categorical_cols, prefix=categorical_cols)

ml_ready_df.to_csv('reddit_retention_ml_ready.csv', index=False)
print("‚úì Saved ML-ready dataset to 'reddit_retention_ml_ready.csv'")

# 3. Create comprehensive summary report
summary_report = f"""
{'='*70}
REDDIT USER RETENTION ANALYSIS - COMPREHENSIVE REPORT
{'='*70}

DATASET OVERVIEW:
{'-'*70}
Total Users Analyzed: {len(user_retention_df):,}
Data Date Range: {user_retention_df['first_activity_date'].min()} to {user_retention_df['last_activity_date'].max()}
Data Span: {(user_retention_df['last_activity_date'].max() - user_retention_df['first_activity_date'].min()).days} days
Total Features: {len(user_retention_df.columns)}

RETENTION METRICS:
{'-'*70}
Mean Retention: {user_retention_df['retention_days'].mean():.2f} days
Median Retention: {user_retention_df['retention_days'].median():.2f} days
Max Retention: {user_retention_df['retention_days'].max():.2f} days
Std Retention: {user_retention_df['retention_days'].std():.2f} days

RETENTION CATEGORY DISTRIBUTION (TARGET VARIABLE):
{'-'*70}
{retention_summary.to_string()}

ENGAGEMENT LEVEL DISTRIBUTION:
{'-'*70}
{engagement_summary.to_string()}

USER TYPE DISTRIBUTION:
{'-'*70}
{user_type_dist.to_string()}

RECENCY STATUS (Current State):
{'-'*70}
{recency_dist.to_string()}

ACTIVITY STATISTICS:
{'-'*70}
Mean Activities per User: {user_retention_df['total_activities'].mean():.2f}
Median Activities per User: {user_retention_df['total_activities'].median():.2f}
Mean Unique Subreddits: {user_retention_df['unique_subreddits'].mean():.2f}
Mean Activity Frequency: {user_retention_df['activity_frequency'].mean():.4f} activities/day
Mean Active Days: {user_retention_df['active_days'].mean():.2f} days

ENGAGEMENT METRICS:
{'-'*70}
Mean Score per Activity: {user_retention_df['avg_score'].mean():.2f}
Mean Total Score: {user_retention_df['total_score'].mean():.2f}

AVERAGE RETENTION BY ENGAGEMENT LEVEL:
{'-'*70}
{avg_retention.to_string()}

CORRELATION WITH RETENTION:
{'-'*70}
{correlations.to_string()}

FEATURES AVAILABLE FOR ML MODELING:
{'-'*70}
{', '.join(user_retention_df.columns.tolist())}

{'='*70}
Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'='*70}
"""

with open('retention_analysis_report.txt', 'w') as f:
    f.write(summary_report)

print("‚úì Saved comprehensive report to 'retention_analysis_report.txt'")

# 4. Save a feature description file
feature_descriptions = """
FEATURE DESCRIPTIONS FOR REDDIT USER RETENTION DATASET

TARGET VARIABLE:
- retention_category: Categorical (short/medium/long/very_long) - Main prediction target
- retention_days: Continuous - Days between first and last activity

TIME FEATURES:
- first_activity_timestamp: Unix timestamp of first activity
- last_activity_timestamp: Unix timestamp of last activity
- first_activity_date: Date of first activity
- last_activity_date: Date of last activity
- days_since_last_activity: Days since user's last activity
- account_age_days: Days since user's first activity

ACTIVITY FEATURES:
- total_activities: Total number of comments + submissions
- num_comments: Number of comments posted
- num_submissions: Number of submissions posted
- unique_subreddits: Number of unique subreddits user participated in
- active_days: Number of unique days user was active
- active_weeks: Number of unique weeks user was active
- active_months: Number of unique months user was active

ENGAGEMENT FEATURES:
- avg_score: Average score across all activities
- max_score: Highest score received on any activity
- total_score: Sum of all scores
- activity_frequency: Activities per day (total_activities / retention_days)
- activities_per_active_day: Activities per day they were actually active
- avg_active_days_per_week: How many days per week user is typically active

RATIO FEATURES:
- comment_submission_ratio: Ratio of comments to submissions

CATEGORICAL FEATURES:
- user_type: commenter/submitter/balanced - Based on comment/submission ratio
- engagement_level: low/medium/high/very_high
- consistency: very_consistent/consistent/sporadic/rare - How regularly user posts
- recency_status: active/recent/inactive/churned - Current activity status
"""

with open('feature_descriptions.txt', 'w') as f:
    f.write(feature_descriptions)

print("‚úì Saved feature descriptions to 'feature_descriptions.txt'")

print("\n" + "="*60)
print("‚úÖ DATA WRANGLING COMPLETE!")
print("="*60)
print(f"\nüìÅ Output Files Created:")
print("  1. reddit_user_retention_features.csv - Main dataset")
print("  2. reddit_user_retention_features.parquet - Compressed version")
print("  3. reddit_retention_ml_ready.csv - ML-ready with encoded features")
print("  4. retention_analysis_comprehensive.png - Visualizations")
print("  5. retention_analysis_report.txt - Detailed analysis report")
print("  6. feature_descriptions.txt - Feature documentation")

print(f"\nüéØ Dataset Summary:")
print(f"  - Total users: {len(user_retention_df):,}")
print(f"  - Total features: {len(user_retention_df.columns)}")
print(f"  - Date range: {(user_retention_df['last_activity_date'].max() - user_retention_df['first_activity_date'].min()).days} days")
print(f"  - Memory usage: {user_retention_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

print(f"\nüìä Target Variable Distribution:")
for category, count in retention_dist.items():
    pct = count / len(user_retention_df) * 100
    print(f"  - {category}: {count:,} ({pct:.1f}%)")

print("\nüöÄ Ready for Machine Learning!")
print("   Next steps: Feature selection, model training, evaluation")

print("\n" + "="*60)
print("ADDITIONAL INSIGHTS & COHORT ANALYSIS")
print("="*60)

# 1. Cohort analysis by join month
print("\nüìÖ COHORT ANALYSIS - Users by Join Month")
print("-"*60)

cohort_analysis = con.execute(f"""
    WITH user_cohorts AS (
        SELECT
            user,
            retention_days,
            retention_category,
            DATE_TRUNC('month', first_activity_date)::DATE as cohort_month,
            total_activities
        FROM (SELECT * FROM user_retention_df)
    )
    SELECT
        cohort_month,
        COUNT(DISTINCT user) as users,
        AVG(retention_days) as avg_retention_days,
        AVG(total_activities) as avg_activities,
        COUNT(CASE WHEN retention_category = 'very_long' THEN 1 END) as long_term_users,
        COUNT(CASE WHEN retention_category = 'short' THEN 1 END) as short_term_users
    FROM user_cohorts
    GROUP BY cohort_month
    ORDER BY cohort_month DESC
    LIMIT 12
""").fetchdf()

print(cohort_analysis.to_string(index=False))

# 2. Subreddit diversity analysis
print("\nüåê SUBREDDIT DIVERSITY ANALYSIS")
print("-"*60)

diversity_analysis = user_retention_df.groupby('unique_subreddits').agg({
    'user': 'count',
    'retention_days': 'mean',
    'total_activities': 'mean',
    'activity_frequency': 'mean'
}).round(2)
diversity_analysis.columns = ['num_users', 'avg_retention', 'avg_activities', 'avg_frequency']
diversity_analysis = diversity_analysis.reset_index()

print("Users by number of unique subreddits visited:")
print(diversity_analysis.head(15).to_string(index=False))

# 3. High-value user segments
print("\nüíé HIGH-VALUE USER SEGMENTS")
print("-"*60)

# Define high-value users (long retention + high engagement)
high_value = user_retention_df[
    (user_retention_df['retention_days'] >= user_retention_df['retention_days'].quantile(0.75)) &
    (user_retention_df['total_activities'] >= user_retention_df['total_activities'].quantile(0.75))
]

print(f"High-value users: {len(high_value):,} ({len(high_value)/len(user_retention_df)*100:.1f}%)")
print(f"Average retention: {high_value['retention_days'].mean():.1f} days")
print(f"Average activities: {high_value['total_activities'].mean():.1f}")
print(f"Average score: {high_value['avg_score'].mean():.2f}")

print("\nCharacteristics of high-value users:")
print("User type distribution:")
print(high_value['user_type'].value_counts())
print("\nConsistency distribution:")
print(high_value['consistency'].value_counts())

# 4. Churn prediction features
print("\n‚ö†Ô∏è CHURN RISK ANALYSIS")
print("-"*60)

# Users who were once active but haven't posted recently
at_risk = user_retention_df[
    (user_retention_df['total_activities'] >= 10) &
    (user_retention_df['days_since_last_activity'] > 30) &
    (user_retention_df['recency_status'].isin(['inactive', 'churned']))
]

print(f"At-risk users: {len(at_risk):,} ({len(at_risk)/len(user_retention_df)*100:.1f}%)")
print(f"Average days since last activity: {at_risk['days_since_last_activity'].mean():.1f}")
print(f"Average historical activities: {at_risk['total_activities'].mean():.1f}")

# 5. Activity patterns over time
print("\nüìà ACTIVITY PATTERNS")
print("-"*60)

activity_patterns = user_retention_df.groupby('consistency').agg({
    'retention_days': ['mean', 'median'],
    'total_activities': 'mean',
    'activity_frequency': 'mean',
    'user': 'count'
}).round(2)

print("Retention and activity by consistency level:")
print(activity_patterns)

# 6. Score impact analysis
print("\n‚≠ê SCORE IMPACT ON RETENTION")
print("-"*60)

# Divide users into score quartiles
user_retention_df['score_quartile'] = pd.qcut(user_retention_df['avg_score'],
                                                q=4,
                                                labels=['Q1_Low', 'Q2', 'Q3', 'Q4_High'],
                                                duplicates='drop')

score_impact = user_retention_df.groupby('score_quartile').agg({
    'retention_days': ['mean', 'median'],
    'total_activities': 'mean',
    'user': 'count'
}).round(2)

print("Retention by average score quartile:")
print(score_impact)

print("\n" + "="*60)

"""## ML"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df_retention_features = pd.read_csv('/content/drive/MyDrive/reddit_analysis/reddit_user_retention_features.csv')
mf_ml_ready = pd.read_csv('/content/drive/MyDrive/reddit_analysis/reddit_retention_ml_ready.csv')

print(df_retention_features.head())
print(df_retention_features.info())
print(mf_ml_ready.head())
print(mf_ml_ready.info())

# ============================================================================
# REDDIT USER RETENTION PREDICTION - PYSPARK MLLIB
# ============================================================================
# Predicting user retention periods in consumer subreddits
# Models: Random Forest, GBT (Gradient Boosted Trees), and Logistic Regression
# Note: PySpark MLlib doesn't have native XGBoost/LightGBM, so we'll use GBT

import os
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import *
from pyspark.ml import Pipeline
from pyspark.ml.feature import (
    VectorAssembler, StringIndexer, OneHotEncoder,
    StandardScaler, MinMaxScaler
)
from pyspark.ml.classification import (
    RandomForestClassifier,
    GBTClassifier,
    LogisticRegression
)
from pyspark.ml.evaluation import (
    MulticlassClassificationEvaluator,
    BinaryClassificationEvaluator
)
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.mllib.evaluation import MulticlassMetrics

# Create directory structure
os.makedirs('code/ml/models', exist_ok=True)
os.makedirs('data/csv', exist_ok=True)
os.makedirs('data/plots', exist_ok=True)

print("="*80)
print("REDDIT USER RETENTION PREDICTION - ML PIPELINE")
print("="*80)
print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

print("\n" + "="*80)
print("1. INITIALIZING SPARK SESSION")
print("="*80)

# Create Spark session with optimized config
spark = SparkSession.builder \
    .appName("RedditRetentionPrediction") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")
print("‚úì Spark session created")
print(f"  Spark version: {spark.version}")

# Load data
print("\n" + "="*80)
print("2. LOADING DATA")
print("="*80)

# Load from Google Drive (update path as needed)
df = spark.read.csv(
    '/content/drive/MyDrive/reddit_analysis/reddit_user_retention_features.csv',
    header=True,
    inferSchema=True
)

print(f"‚úì Data loaded: {df.count():,} users")
print(f"‚úì Columns: {len(df.columns)}")

# Show sample
print("\nSample data:")
df.show(5, truncate=False)

# Data statistics
print("\nRetention category distribution:")
df.groupBy('retention_category').count().orderBy('retention_category').show()

print("\n" + "="*80)
print("3. REDEFINING RETENTION CATEGORIES")
print("="*80)

# Original categories were: short (<7d), medium (7-30d), long (30-90d), very_long (>90d)
# New definitions:
# - short: < 90 days (< 3 months)
# - medium: 90-180 days (3-6 months)
# - long: 180-365 days (6-12 months)
# - very_long: > 365 days (> 12 months)

df = df.withColumn(
    'retention_category_new',
    F.when(F.col('retention_days') < 90, 'short')
     .when((F.col('retention_days') >= 90) & (F.col('retention_days') < 180), 'medium')
     .when((F.col('retention_days') >= 180) & (F.col('retention_days') < 365), 'long')
     .otherwise('very_long')
)

print("New retention category definitions:")
print("  short:      < 3 months (< 90 days)")
print("  medium:     3-6 months (90-180 days)")
print("  long:       6-12 months (180-365 days)")
print("  very_long:  > 12 months (> 365 days)")

print("\nNew distribution:")
df.groupBy('retention_category_new').count().orderBy('retention_category_new').show()

# Replace old category with new
df = df.drop('retention_category').withColumnRenamed('retention_category_new', 'retention_category')

print("\n" + "="*80)
print("4. FEATURE ENGINEERING PIPELINE")
print("="*80)

# Define features to exclude (data leakage or identifiers)
exclude_features = [
    'user', 'retention_days', 'retention_category',
    'last_activity_timestamp', 'last_activity_date',
    'days_since_last_activity', 'recency_status',
    'first_activity_date'
]

# Get numeric and categorical features
numeric_features = [
    'first_activity_timestamp', 'total_activities', 'unique_subreddits',
    'num_comments', 'num_submissions', 'active_days', 'active_weeks',
    'active_months', 'avg_score', 'max_score', 'total_score',
    'account_age_days', 'activity_frequency', 'activities_per_active_day',
    'avg_active_days_per_week', 'comment_submission_ratio'
]

categorical_features = ['user_type', 'engagement_level', 'consistency']

print(f"‚úì Numeric features: {len(numeric_features)}")
print(f"  {numeric_features[:5]}...")
print(f"\n‚úì Categorical features: {len(categorical_features)}")
print(f"  {categorical_features}")

# Handle missing values
print("\nüìä Handling missing values...")
for col in numeric_features:
    median_val = df.approxQuantile(col, [0.5], 0.01)[0]
    df = df.fillna({col: median_val})

for col in categorical_features:
    df = df.fillna({col: 'unknown'})

print("‚úì Missing values handled")

# Create feature engineering stages
stages = []

# 1. String indexing for categorical features
indexers = []
for col in categorical_features:
    indexer = StringIndexer(
        inputCol=col,
        outputCol=f"{col}_indexed",
        handleInvalid='keep'
    )
    indexers.append(indexer)
    stages.append(indexer)

indexed_categorical = [f"{col}_indexed" for col in categorical_features]

# 2. One-hot encoding
encoder = OneHotEncoder(
    inputCols=indexed_categorical,
    outputCols=[f"{col}_encoded" for col in categorical_features],
    dropLast=True
)
stages.append(encoder)

encoded_categorical = [f"{col}_encoded" for col in categorical_features]

# 3. Vector assembler
all_features = numeric_features + encoded_categorical
assembler = VectorAssembler(
    inputCols=all_features,
    outputCol='features_raw',
    handleInvalid='skip'
)
stages.append(assembler)

# 4. Feature scaling
scaler = StandardScaler(
    inputCol='features_raw',
    outputCol='features',
    withMean=True,
    withStd=True
)
stages.append(scaler)

# 5. Label indexing
label_indexer = StringIndexer(
    inputCol='retention_category',
    outputCol='label',
    handleInvalid='keep'
)
stages.append(label_indexer)

print(f"\n‚úì Feature engineering pipeline created with {len(stages)} stages")

print("\n" + "="*80)
print("5. TRAIN-TEST SPLIT")
print("="*80)

# Stratified split by retention category
train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)

print(f"‚úì Training set: {train_df.count():,} users")
print(f"‚úì Test set: {test_df.count():,} users")

print("\nTraining set distribution:")
train_df.groupBy('retention_category').count().orderBy('retention_category').show()

print("Test set distribution:")
test_df.groupBy('retention_category').count().orderBy('retention_category').show()

# Cache the dataframes
train_df.cache()
test_df.cache()

print("\n" + "="*80)
print("6. MODEL TRAINING")
print("="*80)

# Dictionary to store models and results
models = {}
results = {}

# =============================================================================
# MODEL 1: RANDOM FOREST
# =============================================================================
print("\n" + "-"*80)
print("6.1 RANDOM FOREST CLASSIFIER")
print("-"*80)

rf = RandomForestClassifier(
    featuresCol='features',
    labelCol='label',
    predictionCol='prediction',
    probabilityCol='probability',
    numTrees=100,
    maxDepth=10,
    minInstancesPerNode=50,
    seed=42
)

# Create pipeline
rf_pipeline = Pipeline(stages=stages + [rf])

print("‚è≥ Training Random Forest...")
start_time = datetime.now()
rf_model = rf_pipeline.fit(train_df)
training_time = (datetime.now() - start_time).total_seconds()

print(f"‚úì Random Forest trained in {training_time:.2f} seconds ({training_time/60:.1f} minutes)")

# Save model
rf_model.write().overwrite().save('code/ml/models/random_forest_model')
print("‚úì Model saved to code/ml/models/random_forest_model")

models['Random Forest'] = rf_model

# =============================================================================
# MODEL 2: DECISION TREE (Alternative to GBT for multiclass)
# =============================================================================
print("\n" + "-"*80)
print("6.2 DECISION TREE CLASSIFIER")
print("-"*80)

from pyspark.ml.classification import DecisionTreeClassifier

dt = DecisionTreeClassifier(
    featuresCol='features',
    labelCol='label',
    predictionCol='prediction',
    probabilityCol='probability',
    maxDepth=15,
    minInstancesPerNode=50,
    seed=42
)

# Create pipeline
dt_pipeline = Pipeline(stages=stages + [dt])

print("‚è≥ Training Decision Tree...")
start_time = datetime.now()
dt_model = dt_pipeline.fit(train_df)
training_time = (datetime.now() - start_time).total_seconds()

print(f"‚úì Decision Tree trained in {training_time:.2f} seconds ({training_time/60:.1f} minutes)")

# Save model
dt_model.write().overwrite().save('code/ml/models/decision_tree_model')
print("‚úì Model saved to code/ml/models/decision_tree_model")

models['Decision Tree'] = dt_model

# =============================================================================
# MODEL 3: MULTINOMIAL LOGISTIC REGRESSION
# =============================================================================
print("\n" + "-"*80)
print("6.3 MULTINOMIAL LOGISTIC REGRESSION")
print("-"*80)

from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(
    featuresCol='features',
    labelCol='label',
    predictionCol='prediction',
    probabilityCol='probability',
    maxIter=100,
    regParam=0.01,
    elasticNetParam=0.0,
    family='multinomial'
)

# Create pipeline
lr_pipeline = Pipeline(stages=stages + [lr])

print("‚è≥ Training Multinomial Logistic Regression...")
start_time = datetime.now()
lr_model = lr_pipeline.fit(train_df)
training_time = (datetime.now() - start_time).total_seconds()

print(f"‚úì Logistic Regression trained in {training_time:.2f} seconds")

# Save model
lr_model.write().overwrite().save('code/ml/models/logistic_regression_model')
print("‚úì Model saved to code/ml/models/logistic_regression_model")

models['Logistic Regression'] = lr_model

# =============================================================================
# MODEL 4: NAIVE BAYES (Good baseline for multiclass)
# =============================================================================
print("\n" + "-"*80)
print("6.4 NAIVE BAYES CLASSIFIER")
print("-"*80)

from pyspark.ml.classification import NaiveBayes

# Naive Bayes requires non-negative features, so we need MinMaxScaler
# Create alternative pipeline for Naive Bayes
nb_stages = stages[:-2]  # Remove StandardScaler and label indexer

# Add MinMaxScaler instead
minmax_scaler = MinMaxScaler(
    inputCol='features_raw',
    outputCol='features',
    min=0.0,
    max=1.0
)
nb_stages.append(minmax_scaler)

# Add label indexer back
nb_stages.append(label_indexer)

nb = NaiveBayes(
    featuresCol='features',
    labelCol='label',
    predictionCol='prediction',
    probabilityCol='probability',
    smoothing=1.0,
    modelType='multinomial'
)

# Create pipeline
nb_pipeline = Pipeline(stages=nb_stages + [nb])

print("‚è≥ Training Naive Bayes...")
start_time = datetime.now()
nb_model = nb_pipeline.fit(train_df)
training_time = (datetime.now() - start_time).total_seconds()

print(f"‚úì Naive Bayes trained in {training_time:.2f} seconds")

# Save model
nb_model.write().overwrite().save('code/ml/models/naive_bayes_model')
print("‚úì Model saved to code/ml/models/naive_bayes_model")

models['Naive Bayes'] = nb_model

# =============================================================================
# MODEL 5: ONE-VS-REST WITH GRADIENT BOOSTED TREES
# =============================================================================
print("\n" + "-"*80)
print("6.5 ONE-VS-REST GRADIENT BOOSTED TREES")
print("-"*80)

from pyspark.ml.classification import OneVsRest, GBTClassifier

# For One-vs-Rest, we need a binary classifier (GBT)
# GBT will be trained multiple times (once per class)
gbt_base = GBTClassifier(
    featuresCol='features',
    labelCol='label',
    predictionCol='prediction',
    maxIter=50,  # Reduced for speed since it trains multiple times
    maxDepth=8,
    stepSize=0.1,
    seed=42
)

# Wrap in OneVsRest
ovr = OneVsRest(
    featuresCol='features',
    labelCol='label',
    predictionCol='prediction',
    classifier=gbt_base
)

# Create pipeline
ovr_pipeline = Pipeline(stages=stages + [ovr])

print("‚è≥ Training One-vs-Rest GBT (this will take longer)...")
print("   Training 4 separate GBT models (one per class)...")
start_time = datetime.now()
ovr_model = ovr_pipeline.fit(train_df)
training_time = (datetime.now() - start_time).total_seconds()

print(f"‚úì One-vs-Rest GBT trained in {training_time:.2f} seconds ({training_time/60:.1f} minutes)")

# Save model
ovr_model.write().overwrite().save('code/ml/models/ovr_gbt_model')
print("‚úì Model saved to code/ml/models/ovr_gbt_model")

models['One-vs-Rest GBT'] = ovr_model

print("\n‚úì All 5 models trained successfully!")
print(f"\nModels: {list(models.keys())}")

print("\n" + "="*80)
print("7. MODEL EVALUATION")
print("="*80)

# Evaluators
accuracy_evaluator = MulticlassClassificationEvaluator(
    labelCol='label',
    predictionCol='prediction',
    metricName='accuracy'
)

f1_evaluator = MulticlassClassificationEvaluator(
    labelCol='label',
    predictionCol='prediction',
    metricName='f1'
)

precision_evaluator = MulticlassClassificationEvaluator(
    labelCol='label',
    predictionCol='prediction',
    metricName='weightedPrecision'
)

recall_evaluator = MulticlassClassificationEvaluator(
    labelCol='label',
    predictionCol='prediction',
    metricName='weightedRecall'
)

# Evaluate each model
evaluation_results = []
failed_models = []

for model_name, model in models.items():
    print(f"\n{'-'*80}")
    print(f"Evaluating: {model_name}")
    print(f"{'-'*80}")

    try:
        # Make predictions
        start_time = datetime.now()
        predictions = model.transform(test_df)

        # Cache predictions to avoid recomputation
        predictions.cache()

        # Force computation and check for errors
        pred_count = predictions.count()
        print(f"  Generated {pred_count:,} predictions")

        pred_time = (datetime.now() - start_time).total_seconds()

        # Check if predictions have required columns
        if 'prediction' not in predictions.columns:
            raise ValueError(f"Model {model_name} did not generate 'prediction' column")

        # Show sample predictions
        print("  Sample predictions:")
        predictions.select('label', 'prediction').show(5)

        # Calculate metrics with error handling
        try:
            accuracy = accuracy_evaluator.evaluate(predictions)
        except Exception as e:
            print(f"  ‚ö† Warning: Could not calculate accuracy: {e}")
            accuracy = None

        try:
            f1 = f1_evaluator.evaluate(predictions)
        except Exception as e:
            print(f"  ‚ö† Warning: Could not calculate F1: {e}")
            f1 = None

        try:
            precision = precision_evaluator.evaluate(predictions)
        except Exception as e:
            print(f"  ‚ö† Warning: Could not calculate precision: {e}")
            precision = None

        try:
            recall = recall_evaluator.evaluate(predictions)
        except Exception as e:
            print(f"  ‚ö† Warning: Could not calculate recall: {e}")
            recall = None

        # If all metrics failed, use manual calculation
        if accuracy is None:
            print("  Using manual accuracy calculation...")
            correct_predictions = predictions.filter(F.col('label') == F.col('prediction')).count()
            total_predictions = predictions.count()
            accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0.0
            print(f"  Manual accuracy: {accuracy:.4f}")

        # Print results
        if accuracy is not None:
            print(f"‚úì Accuracy:  {accuracy:.4f}")
        if f1 is not None:
            print(f"‚úì F1 Score:  {f1:.4f}")
        if precision is not None:
            print(f"‚úì Precision: {precision:.4f}")
        if recall is not None:
            print(f"‚úì Recall:    {recall:.4f}")
        print(f"‚úì Prediction time: {pred_time:.2f} seconds")

        # Store results
        results[model_name] = {
            'predictions': predictions,
            'accuracy': accuracy if accuracy is not None else 0.0,
            'f1': f1 if f1 is not None else 0.0,
            'precision': precision if precision is not None else 0.0,
            'recall': recall if recall is not None else 0.0,
            'prediction_time': pred_time
        }

        evaluation_results.append({
            'model': model_name,
            'accuracy': accuracy if accuracy is not None else 0.0,
            'f1_score': f1 if f1 is not None else 0.0,
            'precision': precision if precision is not None else 0.0,
            'recall': recall if recall is not None else 0.0,
            'prediction_time_seconds': pred_time
        })

        print(f"‚úì {model_name} evaluation complete")

    except Exception as e:
        print(f"‚úó ERROR evaluating {model_name}: {str(e)}")
        print(f"  Skipping this model...")
        failed_models.append(model_name)
        continue

# Remove failed models from results
for failed_model in failed_models:
    if failed_model in models:
        del models[failed_model]
    if failed_model in results:
        del results[failed_model]

if failed_models:
    print(f"\n‚ö† Warning: {len(failed_models)} model(s) failed: {failed_models}")
    print(f"‚úì Successfully evaluated {len(evaluation_results)} model(s)")

# Save evaluation metrics
if evaluation_results:
    eval_df = pd.DataFrame(evaluation_results)
    eval_df = eval_df.sort_values('accuracy', ascending=False)
    print(f"\n{'='*80}")
    print("MODEL RANKING BY ACCURACY")
    print(f"{'='*80}")
    print(eval_df.to_string(index=False))

    eval_df.to_csv('data/csv/ml_model_evaluation.csv', index=False)
    print("\n‚úì Evaluation metrics saved to data/csv/ml_model_evaluation.csv")

    # Identify best model
    best_model_name = eval_df.iloc[0]['model']
    best_accuracy = eval_df.iloc[0]['accuracy']
    print(f"\nüèÜ BEST MODEL: {best_model_name} (Accuracy: {best_accuracy:.4f})")
else:
    print("\n‚úó ERROR: No models were successfully evaluated!")

print("\n" + "="*80)
print("10. CREATING VISUALIZATIONS")
print("="*80)

# Set style
sns.set_style('whitegrid')

# =============================================================================
# PLOT 1: Model Comparison
# =============================================================================
print("\nüìä Creating model comparison plot...")

fig = plt.figure(figsize=(18, 12))
gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)

# Accuracy comparison
ax1 = fig.add_subplot(gs[0, :])
models_list = eval_df['model'].tolist()
accuracies = eval_df['accuracy'].tolist()
colors = plt.cm.viridis(np.linspace(0, 0.9, len(models_list)))
bars = ax1.barh(models_list, accuracies, color=colors, edgecolor='black', linewidth=1.5)
ax1.set_xlabel('Accuracy', fontsize=12, fontweight='bold')
ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')
ax1.set_xlim([min(accuracies) * 0.95, 1.0])
ax1.axvline(x=best_accuracy, color='red', linestyle='--', linewidth=2, alpha=0.7, label='Best Model')
for i, (bar, acc) in enumerate(zip(bars, accuracies)):
    ax1.text(acc + 0.005, i, f'{acc:.4f}', va='center', fontweight='bold')
ax1.legend()

# F1 Score comparison
ax2 = fig.add_subplot(gs[1, 0])
f1_scores = eval_df['f1_score'].tolist()
bars = ax2.bar(range(len(models_list)), f1_scores, color=colors, edgecolor='black', linewidth=1.5)
ax2.set_ylabel('F1 Score', fontsize=12, fontweight='bold')
ax2.set_xlabel('Model', fontsize=12, fontweight='bold')
ax2.set_title('F1 Score Comparison', fontsize=14, fontweight='bold')
ax2.set_xticks(range(len(models_list)))
ax2.set_xticklabels([m.replace(' ', '\n') for m in models_list], rotation=0, fontsize=9)
ax2.set_ylim([min(f1_scores) * 0.95, 1.0])
for bar, f1 in zip(bars, f1_scores):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,
             f'{f1:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)

# Precision vs Recall
ax3 = fig.add_subplot(gs[1, 1])
precisions = eval_df['precision'].tolist()
recalls = eval_df['recall'].tolist()
scatter = ax3.scatter(recalls, precisions, s=300, alpha=0.6, c=range(len(models_list)), cmap='viridis', edgecolors='black', linewidth=2)
for i, model in enumerate(models_list):
    ax3.annotate(model, (recalls[i], precisions[i]),
                fontsize=8, ha='center', fontweight='bold')
ax3.set_xlabel('Recall', fontsize=12, fontweight='bold')
ax3.set_ylabel('Precision', fontsize=12, fontweight='bold')
ax3.set_title('Precision vs Recall', fontsize=14, fontweight='bold')
ax3.grid(True, alpha=0.3)

# All metrics comparison
ax4 = fig.add_subplot(gs[2, :])
x = np.arange(len(models_list))
width = 0.2
ax4.bar(x - width*1.5, accuracies, width, label='Accuracy', color='#3498db', edgecolor='black')
ax4.bar(x - width*0.5, f1_scores, width, label='F1', color='#2ecc71', edgecolor='black')
ax4.bar(x + width*0.5, precisions, width, label='Precision', color='#e74c3c', edgecolor='black')
ax4.bar(x + width*1.5, recalls, width, label='Recall', color='#f39c12', edgecolor='black')
ax4.set_ylabel('Score', fontsize=12, fontweight='bold')
ax4.set_xlabel('Model', fontsize=12, fontweight='bold')
ax4.set_title('All Metrics Comparison', fontsize=14, fontweight='bold')
ax4.set_xticks(x)
ax4.set_xticklabels([m.replace(' ', '\n') for m in models_list], fontsize=9)
ax4.legend(loc='lower right')
ax4.set_ylim([min(min(accuracies), min(f1_scores), min(precisions), min(recalls)) * 0.95, 1.0])
ax4.grid(axis='y', alpha=0.3)

plt.suptitle('Reddit Retention Prediction - Model Comparison (5 Models)',
             fontsize=16, fontweight='bold', y=0.995)
plt.tight_layout()
plt.savefig('data/plots/ml_model_comparison.png', dpi=300, bbox_inches='tight')
print("‚úì Saved: data/plots/ml_model_comparison.png")
plt.show()

# =============================================================================
# PLOT 2: Confusion Matrices (Top 3 Models)
# =============================================================================
print("\nüìä Creating confusion matrices for top 3 models...")

top_3_models = eval_df.head(3)['model'].tolist()
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for idx, model_name in enumerate(top_3_models):
    ax = axes[idx]

    pred_df = results[model_name]['predictions']
    predictionAndLabels = pred_df.select('prediction', 'label').rdd
    metrics = MulticlassMetrics(predictionAndLabels.map(lambda x: (float(x[0]), float(x[1]))))
    cm = metrics.confusionMatrix().toArray()

    # Normalize
    cm_normalized = cm / cm.sum(axis=1, keepdims=True)

    # Get label names
    label_indexer_model = list(models.values())[0].stages[len(stages)-1]
    labels = label_indexer_model.labels

    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',
                xticklabels=labels, yticklabels=labels,
                ax=ax, cbar_kws={'label': 'Percentage'})

    ax.set_xlabel('Predicted', fontsize=11, fontweight='bold')
    ax.set_ylabel('Actual', fontsize=11, fontweight='bold')
    accuracy = results[model_name]['accuracy']
    ax.set_title(f'{model_name}\nAccuracy: {accuracy:.4f}',
                fontsize=12, fontweight='bold')

plt.suptitle('Confusion Matrices - Top 3 Models', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('data/plots/ml_confusion_matrices.png', dpi=300, bbox_inches='tight')
print("‚úì Saved: data/plots/ml_confusion_matrices.png")
plt.show()

print("\n" + "="*80)
print("10. CREATING ALL REQUIRED VISUALIZATIONS")
print("="*80)

# Set style
sns.set_style('whitegrid')
plt.rcParams['figure.dpi'] = 300

# Get working models (excluding Naive Bayes if it failed)
working_models = {k: v for k, v in models.items() if k != 'Naive Bayes'}
working_results = {k: v for k, v in results.items() if k != 'Naive Bayes'}

print(f"‚úì Creating visualizations for {len(working_models)} models")
print(f"  Models: {list(working_models.keys())}")

# Get label mapping
first_model = list(working_models.values())[0]
label_indexer_model = first_model.stages[len(stages)-1]
label_mapping = {float(i): label for i, label in enumerate(label_indexer_model.labels)}
num_classes = len(label_mapping)

print(f"‚úì Classes: {list(label_mapping.values())}")

# Get feature names from the VectorAssembler (before scaling)
# Find the VectorAssembler in the pipeline stages
assembler_idx = None
for idx, stage in enumerate(first_model.stages):
    if hasattr(stage, 'getInputCols') and 'VectorAssembler' in str(type(stage)):
        assembler_idx = idx
        break

if assembler_idx is not None:
    assembler_model = first_model.stages[assembler_idx]
    feature_names = assembler_model.getInputCols()
else:
    # Fallback: manually reconstruct feature names
    feature_names = numeric_features + [f"{col}_encoded" for col in categorical_features]

print(f"‚úì Found {len(feature_names)} feature names")

# =============================================================================
# PLOT : ONE-VS-REST GBT - AGGREGATE FEATURE IMPORTANCE
# =============================================================================
print("\nüìä Creating One-vs-Rest GBT feature importance plot...")

ovr_classifier = models['One-vs-Rest GBT'].stages[-1]

if hasattr(ovr_classifier, 'models'):
    print("  Aggregating feature importances from 4 binary classifiers...")

    all_importances = []

    for i, binary_model in enumerate(ovr_classifier.models):
        if hasattr(binary_model, 'featureImportances'):
            importances = binary_model.featureImportances.toArray()
            all_importances.append(importances)
            print(f"  ‚úì Extracted importances from binary classifier {i+1}/4 ({len(importances)} features)")

    if all_importances:
        # Average the importances across all binary classifiers
        avg_importances = np.mean(all_importances, axis=0)

        print(f"  Number of averaged importance values: {len(avg_importances)}")

        # Use the expanded feature names from Random Forest
        # Make sure we have enough feature names
        if 'feature_names_to_use' in locals():
            print(f"  Number of feature names available: {len(feature_names_to_use)}")

            # If we need more names, extend the list
            if len(avg_importances) > len(feature_names_to_use):
                ovr_feature_names = feature_names_to_use.copy()
                num_extra = len(avg_importances) - len(feature_names_to_use)
                for i in range(num_extra):
                    ovr_feature_names.append(f'extra_feature_{i+1}')
            else:
                ovr_feature_names = feature_names_to_use[:len(avg_importances)]
        else:
            # Fallback: create generic names
            print("  ‚ö† Using generic feature names")
            ovr_feature_names = [f'feature_{i+1}' for i in range(len(avg_importances))]

        print(f"  ‚úì Using {len(ovr_feature_names)} feature names for {len(avg_importances)} importances")

        # Verify lengths match
        assert len(ovr_feature_names) == len(avg_importances), \
            f"Mismatch: {len(ovr_feature_names)} names vs {len(avg_importances)} importances"

        # Create dataframe
        ovr_importance_df = pd.DataFrame({
            'feature': ovr_feature_names,
            'importance': avg_importances
        }).sort_values('importance', ascending=False)

        # Get top 10
        top_10_ovr = ovr_importance_df.head(10)

        # Save to CSV
        ovr_importance_df.to_csv('data/csv/ml_ovr_gbt_feature_importance_full.csv', index=False)
        print(f"‚úì Saved full feature importance to data/csv/ml_ovr_gbt_feature_importance_full.csv")

        # Create plot
        fig, ax = plt.subplots(figsize=(12,8))

        colors = plt.cm.plasma(np.linspace(0.3, 0.9, len(top_10_ovr)))
        bars = ax.barh(range(len(top_10_ovr)), top_10_ovr['importance'],
                       color=colors, edgecolor='black', linewidth=1.5, alpha=0.85)

        ax.set_yticks(range(len(top_10_ovr)))
        ax.set_yticklabels(top_10_ovr['feature'], fontsize=11, fontweight='bold')
        ax.set_xlabel('Average Importance Score (across 4 binary classifiers)',
                     fontsize=13, fontweight='bold')
        ax.set_title('One-vs-Rest GBT - Top 10 Most Important Features\n(User Retention Prediction)',
                    fontsize=15, fontweight='bold', pad=20)
        ax.invert_yaxis()
        ax.grid(axis='x', alpha=0.3, linestyle='--')
        ax.set_axisbelow(True)

        # Add value labels on bars
        for i, (bar, imp) in enumerate(zip(bars, top_10_ovr['importance'])):
            width = bar.get_width()
            ax.text(width + max(top_10_ovr['importance']) * 0.02, i,
                   f'{imp:.4f}', va='center', fontsize=10, fontweight='bold')

        # Add rank numbers
        for i in range(len(top_10_ovr)):
            ax.text(-max(top_10_ovr['importance']) * 0.05, i,
                   f'#{i+1}', va='center', ha='right', fontsize=10,
                   fontweight='bold', color='darkred')

        plt.tight_layout()
        plt.savefig('data/plots/ml_ovr_gbt_top10_features.png',
                    dpi=300, bbox_inches='tight', facecolor='white')
        print("‚úì Saved One-vs-Rest GBT top 10 plot to data/plots/ml_ovr_gbt_top10_features.png")
        plt.show()

        # Print top 10 features
        print("\nüìã One-vs-Rest GBT - Top 10 Features:")
        print("-" * 70)
        for i, (_, row) in enumerate(top_10_ovr.iterrows(), 1):
            print(f"  {i:2d}. {row['feature']:40s} {row['importance']:.6f}")

    else:
        print("‚ö† Could not extract feature importances from One-vs-Rest GBT models")
else:
    print("‚ö† One-vs-Rest GBT model structure not recognized")
# =============================================================================
# PLOT 1: MODEL COMPARISON CHART
# =============================================================================
print("\nüìä 1. Creating model comparison chart...")

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

models_list = list(working_results.keys())
accuracies = [working_results[m]['accuracy'] for m in models_list]
f1_scores = [working_results[m]['f1'] for m in models_list]
precisions = [working_results[m]['precision'] for m in models_list]
recalls = [working_results[m]['recall'] for m in models_list]

colors = plt.cm.viridis(np.linspace(0, 0.9, len(models_list)))

# Accuracy comparison
ax1 = axes[0, 0]
bars = ax1.barh(models_list, accuracies, color=colors, edgecolor='black', linewidth=1.5)
ax1.set_xlabel('Accuracy', fontsize=12, fontweight='bold')
ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')
ax1.set_xlim([min(accuracies) * 0.95, 1.0])
for i, (bar, acc) in enumerate(zip(bars, accuracies)):
    ax1.text(acc + 0.005, i, f'{acc:.4f}', va='center', fontweight='bold', fontsize=10)

# F1 Score comparison
ax2 = axes[0, 1]
bars = ax2.barh(models_list, f1_scores, color=colors, edgecolor='black', linewidth=1.5)
ax2.set_xlabel('F1 Score', fontsize=12, fontweight='bold')
ax2.set_title('F1 Score Comparison', fontsize=14, fontweight='bold')
ax2.set_xlim([min(f1_scores) * 0.95, 1.0])
for i, (bar, f1) in enumerate(zip(bars, f1_scores)):
    ax2.text(f1 + 0.005, i, f'{f1:.4f}', va='center', fontweight='bold', fontsize=10)

# Precision vs Recall scatter
ax3 = axes[1, 0]
scatter = ax3.scatter(recalls, precisions, s=400, alpha=0.6,
                     c=range(len(models_list)), cmap='viridis',
                     edgecolors='black', linewidth=2)
for i, model in enumerate(models_list):
    ax3.annotate(model, (recalls[i], precisions[i]),
                fontsize=9, ha='center', fontweight='bold')
ax3.set_xlabel('Recall', fontsize=12, fontweight='bold')
ax3.set_ylabel('Precision', fontsize=12, fontweight='bold')
ax3.set_title('Precision vs Recall Trade-off', fontsize=14, fontweight='bold')
ax3.grid(True, alpha=0.3)
ax3.set_xlim([min(recalls) * 0.95, 1.05])
ax3.set_ylim([min(precisions) * 0.95, 1.05])

# All metrics grouped bar chart
ax4 = axes[1, 1]
x = np.arange(len(models_list))
width = 0.2
ax4.bar(x - width*1.5, accuracies, width, label='Accuracy', color='#3498db', edgecolor='black')
ax4.bar(x - width*0.5, f1_scores, width, label='F1', color='#2ecc71', edgecolor='black')
ax4.bar(x + width*0.5, precisions, width, label='Precision', color='#e74c3c', edgecolor='black')
ax4.bar(x + width*1.5, recalls, width, label='Recall', color='#f39c12', edgecolor='black')
ax4.set_ylabel('Score', fontsize=12, fontweight='bold')
ax4.set_xlabel('Model', fontsize=12, fontweight='bold')
ax4.set_title('All Metrics Comparison', fontsize=14, fontweight='bold')
ax4.set_xticks(x)
ax4.set_xticklabels([m.replace(' ', '\n') for m in models_list], fontsize=9)
ax4.legend(loc='lower right', fontsize=10)
ax4.set_ylim([0.5, 1.05])
ax4.grid(axis='y', alpha=0.3)

plt.suptitle('Model Comparison - All Metrics', fontsize=18, fontweight='bold')
plt.tight_layout()
plt.savefig('data/plots/ml_model_comparison.png', dpi=300, bbox_inches='tight')
print("‚úì Saved: data/plots/ml_model_comparison.png")
plt.close()

# =============================================================================
# PLOT 2: CONFUSION MATRICES (ONE PER MODEL)
# =============================================================================
print("\nüìä 2. Creating confusion matrices...")

for model_name in working_models.keys():
    print(f"  Creating confusion matrix for {model_name}...")

    fig, axes = plt.subplots(1, 2, figsize=(14, 6))

    pred_df = working_results[model_name]['predictions']
    predictionAndLabels = pred_df.select('prediction', 'label').rdd
    metrics = MulticlassMetrics(predictionAndLabels.map(lambda x: (float(x[0]), float(x[1]))))
    cm = metrics.confusionMatrix().toArray()

    labels = list(label_mapping.values())

    # Raw counts
    ax1 = axes[0]
    sns.heatmap(cm, annot=True, fmt='.0f', cmap='Blues',
                xticklabels=labels, yticklabels=labels,
                ax=ax1, cbar_kws={'label': 'Count'}, linewidths=0.5)
    ax1.set_xlabel('Predicted', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Actual', fontsize=12, fontweight='bold')
    ax1.set_title('Confusion Matrix (Counts)', fontsize=13, fontweight='bold')

    # Normalized percentages
    ax2 = axes[1]
    cm_normalized = cm / cm.sum(axis=1, keepdims=True)
    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='RdYlGn',
                xticklabels=labels, yticklabels=labels,
                ax=ax2, cbar_kws={'label': 'Percentage'}, linewidths=0.5)
    ax2.set_xlabel('Predicted', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Actual', fontsize=12, fontweight='bold')
    ax2.set_title('Confusion Matrix (Normalized)', fontsize=13, fontweight='bold')

    accuracy = working_results[model_name]['accuracy']
    fig.suptitle(f'{model_name} - Confusion Matrix\nAccuracy: {accuracy:.4f}',
                 fontsize=16, fontweight='bold')
    plt.tight_layout()

    filename = f"ml_{model_name.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')}_confusion_matrix.png"
    plt.savefig(f'data/plots/{filename}', dpi=300, bbox_inches='tight')
    print(f"  ‚úì Saved: data/plots/{filename}")
    plt.close()

# =============================================================================
# PLOT 3: FEATURE IMPORTANCE (FOR TREE-BASED MODELS)
# =============================================================================
print("\nüìä 3. Creating feature importance charts...")

for model_name in working_models.keys():
    try:
        # Get the actual classifier from the pipeline
        classifier = working_models[model_name].stages[-1]

        # Check if model has feature importance
        if hasattr(classifier, 'featureImportances'):
            print(f"  Creating feature importance for {model_name}...")

            importances = classifier.featureImportances.toArray()

            # Make sure we have the right number of features
            if len(importances) != len(feature_names):
                print(f"  ‚ö† Feature count mismatch: {len(importances)} importances vs {len(feature_names)} names")
                # Create generic names
                feature_names_to_use = [f'feature_{i}' for i in range(len(importances))]
            else:
                feature_names_to_use = feature_names

            # Create dataframe
            feat_imp_df = pd.DataFrame({
                'feature': feature_names_to_use,
                'importance': importances
            }).sort_values('importance', ascending=False)

            # Save to CSV
            csv_filename = f"ml_{model_name.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')}_feature_importance.csv"
            feat_imp_df.to_csv(f'data/csv/{csv_filename}', index=False)

            # Plot top 20
            fig, ax = plt.subplots(figsize=(12, 10))
            top_20 = feat_imp_df.head(20)

            bars = ax.barh(range(len(top_20)), top_20['importance'],
                          color='steelblue', alpha=0.8, edgecolor='black', linewidth=1.5)
            ax.set_yticks(range(len(top_20)))
            ax.set_yticklabels(top_20['feature'], fontsize=10)
            ax.set_xlabel('Importance Score', fontsize=12, fontweight='bold')
            ax.set_title(f'Top 20 Most Important Features\n{model_name}',
                        fontsize=14, fontweight='bold')
            ax.invert_yaxis()
            ax.grid(axis='x', alpha=0.3)

            # Add value labels
            for i, (bar, imp) in enumerate(zip(bars, top_20['importance'])):
                ax.text(imp + max(top_20['importance']) * 0.01, i,
                       f'{imp:.4f}', va='center', fontsize=9)

            plt.tight_layout()
            filename = f"ml_{model_name.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')}_feature_importance.png"
            plt.savefig(f'data/plots/{filename}', dpi=300, bbox_inches='tight')
            print(f"  ‚úì Saved: data/plots/{filename}")
            plt.close()

        else:
            print(f"  ‚ö† {model_name} doesn't support feature importance")

    except Exception as e:
        print(f"  ‚ö† Could not extract feature importance for {model_name}: {e}")

# =============================================================================
# PLOT 4: ROC CURVES (ONE-VS-REST FOR MULTICLASS)
# =============================================================================
print("\nüìä 4. Creating ROC curves (One-vs-Rest)...")

from pyspark.sql.types import DoubleType

for model_name in working_models.keys():
    print(f"  Creating ROC curve for {model_name}...")

    try:
        pred_df = working_results[model_name]['predictions']

        # Check if probability column exists
        if 'probability' not in pred_df.columns:
            print(f"  ‚ö† {model_name} doesn't have probability column, skipping ROC curve")
            continue

        fig, ax = plt.subplots(figsize=(10, 8))

        # Calculate ROC for each class (One-vs-Rest)
        auc_scores = []

        for class_idx in range(num_classes):
            class_label = label_mapping[float(class_idx)]

            # Create binary labels (current class vs rest)
            binary_pred_df = pred_df.withColumn(
                'binary_label',
                F.when(F.col('label') == class_idx, 1.0).otherwise(0.0)
            )

            # Extract probability for this class
            extract_prob_udf = F.udf(lambda v: float(v[class_idx]) if v is not None and len(v) > class_idx else 0.0,
                                     DoubleType())
            binary_pred_df = binary_pred_df.withColumn(
                'class_probability',
                extract_prob_udf(F.col('probability'))
            )

            # Collect data for ROC calculation (sample to avoid memory issues)
            sample_size = min(10000, pred_df.count())
            labels_and_scores = binary_pred_df.select('binary_label', 'class_probability') \
                                               .sample(fraction=sample_size/pred_df.count(), seed=42) \
                                               .collect()

            if len(labels_and_scores) > 0:
                y_true = [row['binary_label'] for row in labels_and_scores]
                y_scores = [row['class_probability'] for row in labels_and_scores]

                # Calculate ROC curve
                from sklearn.metrics import roc_curve, auc
                fpr, tpr, _ = roc_curve(y_true, y_scores)
                roc_auc = auc(fpr, tpr)
                auc_scores.append(roc_auc)

                # Plot
                ax.plot(fpr, tpr, linewidth=2,
                       label=f'{class_label} (AUC = {roc_auc:.3f})')

        # Plot diagonal
        ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random (AUC = 0.500)')

        ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')
        ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')
        ax.set_title(f'ROC Curves (One-vs-Rest)\n{model_name}\nMean AUC = {np.mean(auc_scores):.3f}',
                    fontsize=14, fontweight='bold')
        ax.legend(loc='lower right', fontsize=10)
        ax.grid(True, alpha=0.3)
        ax.set_xlim([0, 1])
        ax.set_ylim([0, 1])

        plt.tight_layout()
        filename = f"ml_{model_name.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')}_roc_curve.png"
        plt.savefig(f'data/plots/{filename}', dpi=300, bbox_inches='tight')
        print(f"  ‚úì Saved: data/plots/{filename}")
        plt.close()

    except Exception as e:
        print(f"  ‚ö† Error creating ROC curve for {model_name}: {e}")

# =============================================================================
# PLOT 5: PRECISION-RECALL CURVES (ONE-VS-REST)
# =============================================================================
print("\nüìä 5. Creating Precision-Recall curves...")

for model_name in working_models.keys():
    print(f"  Creating PR curve for {model_name}...")

    try:
        pred_df = working_results[model_name]['predictions']

        if 'probability' not in pred_df.columns:
            print(f"  ‚ö† {model_name} doesn't have probability column, skipping PR curve")
            continue

        fig, ax = plt.subplots(figsize=(10, 8))

        ap_scores = []

        for class_idx in range(num_classes):
            class_label = label_mapping[float(class_idx)]

            # Create binary labels
            binary_pred_df = pred_df.withColumn(
                'binary_label',
                F.when(F.col('label') == class_idx, 1.0).otherwise(0.0)
            )

            # Extract probability
            extract_prob_udf = F.udf(lambda v: float(v[class_idx]) if v is not None and len(v) > class_idx else 0.0,
                                     DoubleType())
            binary_pred_df = binary_pred_df.withColumn(
                'class_probability',
                extract_prob_udf(F.col('probability'))
            )

            # Collect data (sample)
            sample_size = min(10000, pred_df.count())
            labels_and_scores = binary_pred_df.select('binary_label', 'class_probability') \
                                               .sample(fraction=sample_size/pred_df.count(), seed=42) \
                                               .collect()

            if len(labels_and_scores) > 0:
                y_true = [row['binary_label'] for row in labels_and_scores]
                y_scores = [row['class_probability'] for row in labels_and_scores]

                # Calculate PR curve
                from sklearn.metrics import precision_recall_curve, average_precision_score
                precision, recall, _ = precision_recall_curve(y_true, y_scores)
                ap = average_precision_score(y_true, y_scores)
                ap_scores.append(ap)

                # Plot
                ax.plot(recall, precision, linewidth=2,
                       label=f'{class_label} (AP = {ap:.3f})')

        ax.set_xlabel('Recall', fontsize=12, fontweight='bold')
        ax.set_ylabel('Precision', fontsize=12, fontweight='bold')
        ax.set_title(f'Precision-Recall Curves (One-vs-Rest)\n{model_name}\nMean AP = {np.mean(ap_scores):.3f}',
                    fontsize=14, fontweight='bold')
        ax.legend(loc='best', fontsize=10)
        ax.grid(True, alpha=0.3)
        ax.set_xlim([0, 1])
        ax.set_ylim([0, 1])

        plt.tight_layout()
        filename = f"ml_{model_name.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('-', '_')}_precision_recall_curve.png"
        plt.savefig(f'data/plots/{filename}', dpi=300, bbox_inches='tight')
        print(f"  ‚úì Saved: data/plots/{filename}")
        plt.close()

    except Exception as e:
        print(f"  ‚ö† Error creating PR curve for {model_name}: {e}")

# Continue with remaining plots in next message...

# =============================================================================
# PLOT 6: PREDICTION DISTRIBUTION
# =============================================================================
print("\nüìä 6. Creating prediction distribution plots...")

fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.flatten()

# Actual distribution
ax = axes[0]
actual_counts = test_df.groupBy('retention_category').count().toPandas()
actual_counts = actual_counts.sort_values('retention_category')
bars = ax.bar(actual_counts['retention_category'], actual_counts['count'],
              color='gray', alpha=0.7, edgecolor='black', linewidth=2)
ax.set_xlabel('Retention Category', fontsize=11, fontweight='bold')
ax.set_ylabel('Count', fontsize=11, fontweight='bold')
ax.set_title('Actual Distribution\n(Test Set)', fontsize=12, fontweight='bold')
ax.tick_params(axis='x', rotation=45)
ax.grid(axis='y', alpha=0.3)

# Add count labels
for bar in bars:
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width()/2., height,
           f'{int(height):,}', ha='center', va='bottom', fontsize=9)

# Predictions for each model
colors_list = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12', '#9b59b6']
for idx, (model_name, color) in enumerate(zip(working_models.keys(), colors_list)):
    if idx + 1 >= len(axes):
        break

    ax = axes[idx + 1]

    pred_df = working_results[model_name]['predictions']

    # Convert predictions back to labels
    pred_with_labels = pred_df.withColumn(
        'pred_category',
        F.when(F.col('prediction') == 0, label_mapping[0.0])
         .when(F.col('prediction') == 1, label_mapping[1.0])
         .when(F.col('prediction') == 2, label_mapping[2.0])
         .otherwise(label_mapping[3.0])
    )

    pred_counts = pred_with_labels.groupBy('pred_category').count().toPandas()
    pred_counts = pred_counts.sort_values('pred_category')

    bars = ax.bar(pred_counts['pred_category'], pred_counts['count'],
                  color=color, alpha=0.7, edgecolor='black', linewidth=2)
    ax.set_xlabel('Retention Category', fontsize=11, fontweight='bold')
    ax.set_ylabel('Count', fontsize=11, fontweight='bold')
    accuracy = working_results[model_name]['accuracy']
    ax.set_title(f'{model_name}\nAcc: {accuracy:.4f}', fontsize=12, fontweight='bold')
    ax.tick_params(axis='x', rotation=45)
    ax.grid(axis='y', alpha=0.3)

    # Add count labels
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{int(height):,}', ha='center', va='bottom', fontsize=8)

# Hide unused subplots
for idx in range(len(working_models) + 1, len(axes)):
    axes[idx].axis('off')

plt.suptitle('Prediction Distribution - Actual vs Models',
             fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('data/plots/ml_prediction_distribution.png', dpi=300, bbox_inches='tight')
print("‚úì Saved: data/plots/ml_prediction_distribution.png")
plt.close()

# =============================================================================
# PLOT 7: PER-CLASS PERFORMANCE METRICS
# =============================================================================
print("\nüìä 7. Creating per-class performance metrics...")

# Calculate per-class metrics for all models
per_class_data = []

for model_name in working_models.keys():
    pred_df = working_results[model_name]['predictions']
    predictionAndLabels = pred_df.select('prediction', 'label').rdd
    metrics = MulticlassMetrics(predictionAndLabels.map(lambda x: (float(x[0]), float(x[1]))))

    for class_idx in range(num_classes):
        class_label = label_mapping[float(class_idx)]
        precision = metrics.precision(float(class_idx))
        recall = metrics.recall(float(class_idx))
        f1 = metrics.fMeasure(float(class_idx))

        per_class_data.append({
            'model': model_name,
            'class': class_label,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        })

per_class_df = pd.DataFrame(per_class_data)
per_class_df.to_csv('data/csv/ml_per_class_metrics.csv', index=False)

# Plot
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

metrics_to_plot = ['precision', 'recall', 'f1_score']
metric_titles = ['Precision', 'Recall', 'F1-Score']

for idx, (metric, title) in enumerate(zip(metrics_to_plot, metric_titles)):
    ax = axes[idx]

    # Prepare data for grouped bar chart
    classes = list(label_mapping.values())
    x = np.arange(len(classes))
    width = 0.8 / len(working_models)

    for model_idx, model_name in enumerate(working_models.keys()):
        model_data = per_class_df[per_class_df['model'] == model_name]
        values = [model_data[model_data['class'] == c][metric].values[0]
                 if len(model_data[model_data['class'] == c]) > 0 else 0
                 for c in classes]

        offset = width * (model_idx - len(working_models)/2 + 0.5)
        ax.bar(x + offset, values, width, label=model_name,
              color=colors_list[model_idx], alpha=0.8, edgecolor='black')

    ax.set_ylabel(title, fontsize=12, fontweight='bold')
    ax.set_xlabel('Retention Category', fontsize=12, fontweight='bold')
    ax.set_title(f'{title} by Class', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(classes, rotation=45, ha='right')
    ax.legend(fontsize=9, loc='best')
    ax.set_ylim([0, 1.05])
    ax.grid(axis='y', alpha=0.3)

plt.suptitle('Per-Class Performance Metrics', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('data/plots/ml_per_class_performance.png', dpi=300, bbox_inches='tight')
print("‚úì Saved: data/plots/ml_per_class_performance.png")
plt.close()

# =============================================================================
# PLOT 8: LEARNING CURVES (Using training history if available)
# =============================================================================
print("\nüìä 8. Creating learning curves...")

# Note: Spark MLlib doesn't easily provide training history like sklearn
# We'll create a simulated learning curve by training on different data sizes

print("  Generating learning curve data (training on different data sizes)...")

learning_curve_data = []

# Use subsets: 20%, 40%, 60%, 80%, 100%
train_fractions = [0.2, 0.4, 0.6, 0.8, 1.0]

# We'll do this for one representative model (Random Forest) to save time
print("  Creating learning curve for Random Forest...")

from pyspark.ml.classification import RandomForestClassifier

for fraction in train_fractions:
    print(f"    Training on {fraction*100:.0f}% of data...")

    # Sample training data
    if fraction < 1.0:
        train_subset = train_df.sample(fraction=fraction, seed=42)
    else:
        train_subset = train_df

    # Train model
    rf_temp = RandomForestClassifier(
        featuresCol='features',
        labelCol='label',
        numTrees=50,  # Fewer trees for speed
        maxDepth=10,
        seed=42
    )

    pipeline_temp = Pipeline(stages=stages + [rf_temp])
    model_temp = pipeline_temp.fit(train_subset)

    # Evaluate on training set
    train_pred = model_temp.transform(train_subset)
    train_acc = accuracy_evaluator.evaluate(train_pred)

    # Evaluate on test set
    test_pred = model_temp.transform(test_df)
    test_acc = accuracy_evaluator.evaluate(test_pred)

    learning_curve_data.append({
        'train_size': int(fraction * train_df.count()),
        'train_fraction': fraction,
        'train_accuracy': train_acc,
        'test_accuracy': test_acc
    })

# Plot learning curve
fig, ax = plt.subplots(figsize=(10, 6))

lc_df = pd.DataFrame(learning_curve_data)
ax.plot(lc_df['train_fraction'] * 100, lc_df['train_accuracy'],
       'o-', linewidth=2, markersize=8, label='Training Accuracy', color='#2ecc71')
ax.plot(lc_df['train_fraction'] * 100, lc_df['test_accuracy'],
       'o-', linewidth=2, markersize=8, label='Test Accuracy', color='#e74c3c')

ax.set_xlabel('Training Set Size (%)', fontsize=12, fontweight='bold')
ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')
ax.set_title('Learning Curve - Random Forest\n(Performance vs Training Data Size)',
            fontsize=14, fontweight='bold')
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3)
ax.set_xlim([15, 105])
ax.set_ylim([0.5, 1.05])

# Add value labels
for _, row in lc_df.iterrows():
    ax.text(row['train_fraction'] * 100, row['train_accuracy'] + 0.02,
           f"{row['train_accuracy']:.3f}", ha='center', fontsize=9)
    ax.text(row['train_fraction'] * 100, row['test_accuracy'] - 0.02,
           f"{row['test_accuracy']:.3f}", ha='center', fontsize=9)

plt.tight_layout()
plt.savefig('data/plots/ml_random_forest_learning_curve.png', dpi=300, bbox_inches='tight')
print("‚úì Saved: data/plots/ml_random_forest_learning_curve.png")
plt.close()

print("\n" + "="*80)
print("‚úÖ ALL VISUALIZATIONS CREATED SUCCESSFULLY")
print("="*80)

# Print summary
print("\nüìä VISUALIZATION SUMMARY:")
print("-" * 80)
viz_files = [
    "ml_model_comparison.png",
    "ml_[model]_confusion_matrix.png (one per model)",
    "ml_[model]_feature_importance.png (for tree-based models)",
    "ml_[model]_roc_curve.png (one per model)",
    "ml_[model]_precision_recall_curve.png (one per model)",
    "ml_prediction_distribution.png",
    "ml_per_class_performance.png",
    "ml_random_forest_learning_curve.png"
]

for viz in viz_files:
    print(f"  ‚úì {viz}")

print(f"\nüìÅ All visualizations saved to: data/plots/")
print(f"üìä All CSV metrics saved to: data/csv/")









"""# bottom"""



































