# -*- coding: utf-8 -*-
"""Analytics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12FB_ka2uPFl7w1dwvgl5ViIljyvVwyFN
"""

"""
Coptic-English Translation Quality Analysis Toolkit
A comprehensive suite of functions for analyzing machine translation quality
"""

import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from typing import Optional, List, Tuple, Dict
import warnings
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Suppress warnings
warnings.filterwarnings('ignore')

# Download required NLTK data
try:
    nltk.data.find('corpora/stopwords')
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
    nltk.download('punkt_tab', quiet=True)

# Set plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (15, 10)

# ============================================================================
# 1. DATA CLEANING AND PREPROCESSING
# ============================================================================

def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Standardize column names for consistency.
    - 'target' -> 'reference'
    - 'prediction' -> 'predictions'

    Args:
        df: Input DataFrame

    Returns:
        DataFrame with standardized column names
    """
    df = df.copy()

    # Rename columns
    rename_dict = {}
    if 'target' in df.columns and 'reference' not in df.columns:
        rename_dict['target'] = 'reference'
    if 'prediction' in df.columns and 'predictions' not in df.columns:
        rename_dict['prediction'] = 'predictions'

    if rename_dict:
        df = df.rename(columns=rename_dict)
        print(f"Renamed columns: {rename_dict}")

    return df


def clean_predictions_and_references(df: pd.DataFrame) -> pd.DataFrame:
    """
    Clean prediction and reference columns.
    - Remove b'...' encoding artifacts
    - Remove [HERE BEGINNETH] markers

    Args:
        df: Input DataFrame

    Returns:
        Cleaned DataFrame
    """
    df = df.copy()

    # Clean predictions
    if 'predictions' in df.columns:
        df['predictions'] = df['predictions'].apply(
            lambda x: x.strip("b'\"") if isinstance(x, str) else x
        )

    # Clean references
    if 'reference' in df.columns:
        df['reference'] = df['reference'].apply(
            lambda x: x.replace('[HERE BEGINNETH]', '').strip() if isinstance(x, str) else x
        )

    print("Cleaned predictions and references")
    return df


def remove_duplicate_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    Remove duplicate columns (e.g., source.1, prediction.1).

    Args:
        df: Input DataFrame

    Returns:
        DataFrame with duplicates removed
    """
    df = df.copy()

    # Find columns ending with .1, .2, etc.
    cols_to_drop = [col for col in df.columns if re.match(r'.*\.\d+$', col)]

    if cols_to_drop:
        df = df.drop(columns=cols_to_drop, errors='ignore')
        print(f"Removed duplicate columns: {cols_to_drop}")

    return df


def add_foreign_language_totals(df: pd.DataFrame) -> pd.DataFrame:
    """
    Add total foreign language token count column.

    Args:
        df: Input DataFrame with foreign language columns

    Returns:
        DataFrame with total_foreign_tokens column
    """
    df = df.copy()

    foreign_lang_cols = ['n_grc_loantokens', 'n_lat_loantokens', 'n_egy_loantokens',
                         'n_heb_loantokens', 'n_arc_loantokens']

    # Check which columns exist
    existing_cols = [col for col in foreign_lang_cols if col in df.columns]

    if existing_cols:
        df['total_foreign_tokens'] = df[existing_cols].sum(axis=1)
        print(f"Added total_foreign_tokens from: {existing_cols}")

    return df


def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    Complete preprocessing pipeline.

    Args:
        df: Input DataFrame

    Returns:
        Fully preprocessed DataFrame
    """
    print("="*80)
    print("PREPROCESSING DATA")
    print("="*80)

    df = standardize_columns(df)
    df = clean_predictions_and_references(df)
    df = remove_duplicate_columns(df)
    df = add_foreign_language_totals(df)

    print("\nPreprocessing complete!")
    print(f"Final shape: {df.shape}")
    return df


# ============================================================================
# 2. METRIC CALCULATION
# ============================================================================

def add_sentence_level_metrics(
    df: pd.DataFrame,
    src_col_candidates: Tuple[str, ...] = ("source", "Coptic", "src"),
    ref_col_candidates: Tuple[str, ...] = ("reference", "target_text", "English_ref", "English"),
    pred_col_candidates: Tuple[str, ...] = ("predictions", "pred", "English_pred"),
    comet_model_id: str = "unbabel/wmt22-comet-da",
    comet_batch: int = 64,
    bert_lang: str = "en"
) -> pd.DataFrame:
    """
    Add comprehensive sentence-level metrics: BLEU, COMET, chrF-S, WER, BERTScore, chrF++.

    Args:
        df: Input DataFrame
        src_col_candidates: Possible source column names
        ref_col_candidates: Possible reference column names
        pred_col_candidates: Possible prediction column names
        comet_model_id: COMET model identifier
        comet_batch: Batch size for COMET computation
        bert_lang: Language for BERTScore

    Returns:
        DataFrame with added metric columns
    """
    import sacrebleu
    from jiwer import wer as jiwer_wer
    import evaluate
    from tqdm import tqdm

    # Map columns
    cols = df.columns.tolist()

    def find_col(cands):
        for c in cands:
            if c in cols:
                return c
        return None

    src_col = find_col(src_col_candidates)
    ref_col = find_col(ref_col_candidates)
    pred_col = find_col(pred_col_candidates)

    if src_col is None or ref_col is None or pred_col is None:
        raise KeyError(f"Could not find required columns. Found: {cols}\n"
                      f"Looking for source in {src_col_candidates}, ref in {ref_col_candidates}, pred in {pred_col_candidates}")

    # Ensure strings
    df = df.copy().reset_index(drop=True)
    df[src_col] = df[src_col].astype(str)
    df[ref_col] = df[ref_col].astype(str)
    df[pred_col] = df[pred_col].astype(str)

    n = len(df)
    print(f"\nDetected columns -> source: '{src_col}', reference: '{ref_col}', prediction: '{pred_col}'")
    print(f"Number of rows: {n}")

    sources = df[src_col].tolist()
    refs = df[ref_col].tolist()
    preds = df[pred_col].tolist()

    # 1. Sentence-level BLEU
    print("Computing sentence-level BLEU...")
    bleu_scores = []
    for p, r in zip(preds, refs):
        try:
            bleu_obj = sacrebleu.sentence_bleu(p, [r])
            bleu_scores.append(float(bleu_obj.score))
        except Exception:
            bleu_scores.append(float("nan"))

    # 2. WER
    print("Computing WER...")
    wer_scores = []
    for p, r in zip(preds, refs):
        try:
            wer_scores.append(float(jiwer_wer(r, p)))
        except Exception:
            wer_scores.append(float("nan"))

    # 3. chrF-S and chrF++
    print("Computing chrF-S and chrF++...")
    chrf = evaluate.load("chrf")
    chrf_s_scores = []
    chrfpp_scores = []
    for p, r in zip(preds, refs):
        try:
            out_s = chrf.compute(predictions=[p], references=[[r]], char_order=6, word_order=2)
            chrf_s_scores.append(float(out_s["score"]) / 100.0)
        except Exception:
            chrf_s_scores.append(float("nan"))
        try:
            out_pp = chrf.compute(predictions=[p], references=[[r]], word_order=2)
            chrfpp_scores.append(float(out_pp["score"]))
        except Exception:
            chrfpp_scores.append(float("nan"))

    # 4. BERTScore
    print("Computing BERTScore...")
    bert = evaluate.load("bertscore")
    try:
        bert_out = bert.compute(predictions=preds, references=refs, lang=bert_lang)
        bs_p = [float(x) for x in bert_out["precision"]]
        bs_r = [float(x) for x in bert_out["recall"]]
        bs_f1 = [float(x) for x in bert_out["f1"]]
    except Exception as e:
        print("BERTScore failed:", e)
        bs_p = [float("nan")] * n
        bs_r = [float("nan")] * n
        bs_f1 = [float("nan")] * n

    # 5. COMET
    print("Computing COMET...")
    comet_metric = None
    comet_scores = [float("nan")] * n
    try:
        comet_metric = evaluate.load("comet", model_id=comet_model_id)
    except Exception as e:
        print("Failed to load COMET:", e)

    if comet_metric is not None:
        all_seg_scores = []
        for i in tqdm(range(0, n, comet_batch), desc="COMET batches"):
            chunk_preds = preds[i:i+comet_batch]
            chunk_refs = refs[i:i+comet_batch]
            chunk_srcs = sources[i:i+comet_batch]
            try:
                out = comet_metric.compute(predictions=chunk_preds, references=chunk_refs, sources=chunk_srcs)
                if "scores" in out:
                    chunk_scores = list(out["scores"])
                elif "score" in out:
                    chunk_scores = [float(out["score"])] * len(chunk_preds)
                else:
                    chunk_scores = [float("nan")] * len(chunk_preds)
            except Exception as e:
                print(f"COMET chunk failed at {i}:{i+comet_batch} -> {e}")
                chunk_scores = [float("nan")] * len(chunk_preds)
            all_seg_scores.extend(chunk_scores)

        if len(all_seg_scores) < n:
            all_seg_scores += [float("nan")] * (n - len(all_seg_scores))
        comet_scores = all_seg_scores[:n]

    # Add to dataframe
    df_out = df.copy()
    df_out["BLEU"] = bleu_scores
    df_out["COMET"] = comet_scores
    df_out["chrF-S"] = chrf_s_scores
    df_out["WER"] = wer_scores
    df_out["BERTScore_P"] = bs_p
    df_out["BERTScore_R"] = bs_r
    df_out["BERTScore_F1"] = bs_f1
    df_out["chrF++"] = chrfpp_scores

    print("Done! Added columns: BLEU, COMET, chrF-S, WER, BERTScore_P/R/F1, chrF++")
    return df_out


# ============================================================================
# 3. FEATURE ENGINEERING
# ============================================================================

def extract_text_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Extract comprehensive text properties for analysis.

    Args:
        df: Input DataFrame with source, reference, and predictions columns

    Returns:
        DataFrame with added feature columns
    """
    df = df.copy()

    # Length features
    df['source_length'] = df['source'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)
    df['reference_length'] = df['reference'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)
    df['prediction_length'] = df['predictions'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)

    # Word count features
    df['source_word_count'] = df['source'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)
    df['reference_word_count'] = df['reference'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)
    df['prediction_word_count'] = df['predictions'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)

    # Average word length
    df['source_avg_word_len'] = df['source'].apply(
        lambda x: np.mean([len(w) for w in str(x).split()]) if pd.notna(x) and len(str(x).split()) > 0 else 0
    )
    df['reference_avg_word_len'] = df['reference'].apply(
        lambda x: np.mean([len(w) for w in str(x).split()]) if pd.notna(x) and len(str(x).split()) > 0 else 0
    )
    df['prediction_avg_word_len'] = df['predictions'].apply(
        lambda x: np.mean([len(w) for w in str(x).split()]) if pd.notna(x) and len(str(x).split()) > 0 else 0
    )

    # Ratios
    df['length_ratio'] = df['prediction_length'] / df['reference_length'].replace(0, 1)
    df['word_count_ratio'] = df['prediction_word_count'] / df['reference_word_count'].replace(0, 1)

    # Punctuation counts
    df['source_punct_count'] = df['source'].apply(lambda x: len(re.findall(r'[^\w\s]', str(x))) if pd.notna(x) else 0)
    df['reference_punct_count'] = df['reference'].apply(lambda x: len(re.findall(r'[^\w\s]', str(x))) if pd.notna(x) else 0)
    df['prediction_punct_count'] = df['predictions'].apply(lambda x: len(re.findall(r'[^\w\s]', str(x))) if pd.notna(x) else 0)

    # Unique words
    df['prediction_unique_words'] = df['predictions'].apply(
        lambda x: len(set(str(x).lower().split())) if pd.notna(x) else 0
    )
    df['prediction_lexical_diversity'] = df['prediction_unique_words'] / df['prediction_word_count'].replace(0, 1)

    print("Extracted text features")
    return df


def categorize_quality(df: pd.DataFrame,
                       metric: str = 'BERTScore_F1',
                       bins: List[float] = [0, 0.80, 0.85, 1.0],
                       labels: List[str] = ['Poor', 'Medium', 'Good']) -> pd.DataFrame:
    """
    Categorize translation quality based on a metric.

    Args:
        df: Input DataFrame
        metric: Metric column to use for categorization
        bins: Bin edges for categories
        labels: Category labels

    Returns:
        DataFrame with 'quality' column added
    """
    df = df.copy()
    df['quality'] = pd.cut(df[metric], bins=bins, labels=labels)

    print(f"\nQuality categorization based on {metric}:")
    print(df['quality'].value_counts().sort_index())

    return df


# ============================================================================
# 4. WORD FREQUENCY ANALYSIS
# ============================================================================

def get_word_frequency(texts: List[str], remove_stopwords: bool = True) -> Counter:
    """
    Extract word frequencies from a list of texts.

    Args:
        texts: List of text strings
        remove_stopwords: Whether to remove stopwords

    Returns:
        Counter object with word frequencies
    """
    stop_words = set(stopwords.words('english'))
    all_words = []

    for text in texts:
        if pd.notna(text):
            text = text.lower()
            text = re.sub(r'[^\w\s]', '', text)
            words = word_tokenize(text)

            if remove_stopwords:
                words = [w for w in words if w not in stop_words and len(w) > 2]
            else:
                words = [w for w in words if len(w) > 2]

            all_words.extend(words)

    return Counter(all_words)


def plot_word_frequency_comparison(df: pd.DataFrame,
                                   metric: str = 'BERTScore_F1',
                                   good_threshold: float = 0.85,
                                   poor_threshold: float = 0.80,
                                   top_n: int = 20,
                                   save_path: Optional[str] = None):
    """
    Plot word frequency comparison between good and poor translations.

    Args:
        df: Input DataFrame
        metric: Metric to use for categorization
        good_threshold: Threshold for good translations
        poor_threshold: Threshold for poor translations
        top_n: Number of top words to display
        save_path: Path to save the figure
    """
    good_translations = df[df[metric] >= good_threshold]['predictions'].tolist()
    poor_translations = df[df[metric] < poor_threshold]['predictions'].tolist()

    print(f"Good translations ({metric} >= {good_threshold}): {len(good_translations)}")
    print(f"Poor translations ({metric} < {poor_threshold}): {len(poor_translations)}")

    good_word_freq = get_word_frequency(good_translations)
    poor_word_freq = get_word_frequency(poor_translations)

    top_good_words = dict(good_word_freq.most_common(top_n))
    top_poor_words = dict(poor_word_freq.most_common(top_n))

    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # Good translations
    axes[0].barh(list(top_good_words.keys()), list(top_good_words.values()), color='#10b981')
    axes[0].set_xlabel('Frequency', fontsize=12, fontweight='bold')
    axes[0].set_title(f'Top {top_n} Words in Good Translations\n({metric} ≥ {good_threshold})',
                      fontsize=14, fontweight='bold', color='#059669')
    axes[0].invert_yaxis()
    axes[0].grid(axis='x', alpha=0.3)

    # Poor translations
    axes[1].barh(list(top_poor_words.keys()), list(top_poor_words.values()), color='#ef4444')
    axes[1].set_xlabel('Frequency', fontsize=12, fontweight='bold')
    axes[1].set_title(f'Top {top_n} Words in Poor Translations\n({metric} < {poor_threshold})',
                      fontsize=14, fontweight='bold', color='#dc2626')
    axes[1].invert_yaxis()
    axes[1].grid(axis='x', alpha=0.3)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

    plt.show()


# ============================================================================
# 5. CORRELATION ANALYSIS
# ============================================================================

def plot_feature_correlations(df: pd.DataFrame,
                              target_metrics: List[str] = ['BERTScore_F1', 'BLEU', 'COMET', 'WER'],
                              save_path: Optional[str] = None):
    """
    Plot correlation heatmaps between text features and target metrics.

    Args:
        df: Input DataFrame with features and metrics
        target_metrics: List of target metrics to analyze
        save_path: Path to save the figure
    """
    feature_cols = [
        'source_length', 'reference_length', 'prediction_length',
        'source_word_count', 'reference_word_count', 'prediction_word_count',
        'source_avg_word_len', 'reference_avg_word_len', 'prediction_avg_word_len',
        'length_ratio', 'word_count_ratio',
        'prediction_unique_words', 'prediction_lexical_diversity'
    ]

    # Filter to existing columns
    feature_cols = [col for col in feature_cols if col in df.columns]
    target_metrics = [col for col in target_metrics if col in df.columns]

    if not feature_cols or not target_metrics:
        print("Missing required columns for correlation analysis")
        return

    fig, axes = plt.subplots(2, 2, figsize=(18, 16))

    for idx, metric in enumerate(target_metrics[:4]):
        ax = axes[idx // 2, idx % 2]

        corr_data = df[feature_cols + [metric]].corr()[metric].drop(metric).sort_values(ascending=False)

        colors = ['#10b981' if x > 0 else '#ef4444' for x in corr_data.values]

        bars = ax.barh(range(len(corr_data)), corr_data.values, color=colors, edgecolor='black')
        ax.set_yticks(range(len(corr_data)))
        ax.set_yticklabels(corr_data.index, fontsize=10)
        ax.set_xlabel('Correlation Coefficient', fontsize=11, fontweight='bold')
        ax.set_title(f'Feature Correlations with {metric}', fontsize=13, fontweight='bold')
        ax.axvline(0, color='black', linewidth=2)
        ax.grid(axis='x', alpha=0.3)

        for i, (bar, val) in enumerate(zip(bars, corr_data.values)):
            ax.text(val + 0.01 if val > 0 else val - 0.01, i, f'{val:.3f}',
                   va='center', ha='left' if val > 0 else 'right', fontweight='bold', fontsize=9)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

    plt.show()


# ============================================================================
# 6. COMPARISON ANALYSIS
# ============================================================================

def compare_bertscore_distributions(df1: pd.DataFrame,
                                   df2: pd.DataFrame,
                                   label1: str = "Baseline",
                                   label2: str = "Improved",
                                   save_path: Optional[str] = None):
    """
    Compare BERTScore distributions between two dataframes.

    Args:
        df1: First DataFrame
        df2: Second DataFrame
        label1: Label for first dataset
        label2: Label for second dataset
        save_path: Path to save the figure
    """
    metrics = ["BERTScore_P", "BERTScore_R", "BERTScore_F1"]

    # Ensure metrics exist
    for df in [df1, df2]:
        for m in metrics:
            df[m] = pd.to_numeric(df[m], errors='coerce')

    # Compute means
    mean1 = df1[metrics].mean()
    mean2 = df2[metrics].mean()

    vals1 = mean1.values
    vals2 = mean2.values

    # Bar plot comparison
    x = np.arange(len(metrics))
    width = 0.35

    plt.figure(figsize=(8, 5))
    plt.bar(x - width/2, vals1, width, label=label1, alpha=0.85)
    plt.bar(x + width/2, vals2, width, label=label2, alpha=0.85)

    plt.xticks(x, metrics, fontsize=12)
    plt.ylabel("Score", fontsize=13)
    plt.title("Comparison of BERTScore Metrics", fontsize=16)
    plt.grid(axis='y', alpha=0.3)
    plt.legend(fontsize=12)

    for i, v in enumerate(vals1):
        plt.text(i - width/2, v + 0.001, f"{v:.3f}", ha='center', fontsize=10)
    for i, v in enumerate(vals2):
        plt.text(i + width/2, v + 0.001, f"{v:.3f}", ha='center', fontsize=10)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')

    plt.show()

    # Print statistics
    bs1 = df1['BERTScore_F1'].dropna()
    bs2 = df2['BERTScore_F1'].dropna()

    mean1_f1 = bs1.mean()
    mean2_f1 = bs2.mean()

    absolute_increase = mean2_f1 - mean1_f1
    percentage_increase = (absolute_increase / mean1_f1) * 100 if mean1_f1 != 0 else np.nan

    print("="*80)
    print("BERTSCORE F1 COMPARISON")
    print("="*80)
    print(f"{label1} mean BERTScore F1:       {mean1_f1:.4f}")
    print(f"{label2} mean BERTScore F1:       {mean2_f1:.4f}")
    print(f"Absolute Increase:                {absolute_increase:.4f}")
    print(f"Percentage Increase:              {percentage_increase:.2f}%")


def compare_bible_vs_nonbible(df: pd.DataFrame,
                              bible_col: str = 'from_bible',
                              save_path: Optional[str] = None):
    """
    Compare translation quality between Bible and non-Bible sources.

    Args:
        df: Input DataFrame with from_bible column
        bible_col: Name of the column indicating Bible source
        save_path: Path to save the figure
    """
    if bible_col not in df.columns:
        print(f"Column '{bible_col}' not found in DataFrame")
        return

    bible_samples = df[df[bible_col] == True]
    non_bible_samples = df[df[bible_col] == False]

    print("="*80)
    print("BIBLE VS NON-BIBLE COMPARISON")
    print("="*80)
    print(f"Bible samples: {len(bible_samples)} ({len(bible_samples)/len(df)*100:.1f}%)")
    print(f"Non-Bible samples: {len(non_bible_samples)} ({len(non_bible_samples)/len(df)*100:.1f}%)")

    metrics = ['BERTScore_F1', 'BERTScore_P', 'BERTScore_R', 'BLEU', 'chrF-S', 'WER', 'chrF++']
    metrics = [m for m in metrics if m in df.columns]

    print("\n" + "-"*80)
    print("TRANSLATION QUALITY METRICS COMPARISON")
    print("-"*80)
    print(f"{'Metric':<15} {'Bible Mean':<12} {'Non-Bible Mean':<15} {'Difference':<12} {'P-value':<12} {'Significant?'}")
    print("-"*80)

    for metric in metrics:
        bible_mean = bible_samples[metric].mean()
        non_bible_mean = non_bible_samples[metric].mean()
        difference = bible_mean - non_bible_mean

        t_stat, p_value = stats.ttest_ind(bible_samples[metric], non_bible_samples[metric])

        significant = "Yes***" if p_value < 0.001 else ("Yes**" if p_value < 0.01 else ("Yes*" if p_value < 0.05 else "No"))

        print(f"{metric:<15} {bible_mean:<12.4f} {non_bible_mean:<15.4f} {difference:<12.4f} {p_value:<12.6f} {significant}")

"""
Complete Usage Example for Coptic Translation Analysis Toolkit
Shows how to use all the functions together
"""

# ============================================================================
# EXAMPLE USAGE PIPELINE
# ============================================================================

def analyze_translation_dataset(
    csv_path: str,
    calculate_metrics: bool = True,
    save_outputs: bool = True,
    output_dir: str = "./output"
) -> pd.DataFrame:
    """
    Complete analysis pipeline for a translation dataset.

    Args:
        csv_path: Path to input CSV file
        calculate_metrics: Whether to calculate metrics (requires packages)
        save_outputs: Whether to save visualizations
        output_dir: Directory for output files

    Returns:
        Fully analyzed DataFrame
    """
    import os
    from coptic_translation_analysis import (
        preprocess_dataframe,
        add_sentence_level_metrics,
        extract_text_features,
        categorize_quality,
        plot_word_frequency_comparison,
        plot_feature_correlations,
        compare_bible_vs_nonbible
    )

    # Create output directory
    if save_outputs:
        os.makedirs(output_dir, exist_ok=True)

    print("="*80)
    print("COPTIC TRANSLATION ANALYSIS PIPELINE")
    print("="*80)

    # Step 1: Load and preprocess
    print("\n[1/6] Loading and preprocessing data...")
    df = pd.read_csv(csv_path)
    print(f"Loaded {len(df)} rows, {len(df.columns)} columns")

    df = preprocess_dataframe(df)

    # Step 2: Calculate metrics (if needed)
    if calculate_metrics and 'BERTScore_F1' not in df.columns:
        print("\n[2/6] Calculating sentence-level metrics...")
        try:
            df = add_sentence_level_metrics(df)
        except Exception as e:
            print(f"Error calculating metrics: {e}")
            print("Continuing with existing metrics...")
    else:
        print("\n[2/6] Using existing metrics")

    # Step 3: Extract features
    print("\n[3/6] Extracting text features...")
    df = extract_text_features(df)

    # Step 4: Categorize quality
    print("\n[4/6] Categorizing translation quality...")
    df = categorize_quality(df)

    # Step 5: Word frequency analysis
    print("\n[5/6] Analyzing word frequencies...")
    save_path = f"{output_dir}/word_frequency.png" if save_outputs else None
    plot_word_frequency_comparison(df, save_path=save_path)

    # Step 6: Correlation analysis
    print("\n[6/6] Analyzing feature correlations...")
    save_path = f"{output_dir}/correlations.png" if save_outputs else None
    plot_feature_correlations(df, save_path=save_path)

    # Optional: Bible vs non-Bible comparison
    if 'from_bible' in df.columns:
        print("\n[BONUS] Comparing Bible vs Non-Bible sources...")
        save_path = f"{output_dir}/bible_comparison.png" if save_outputs else None
        compare_bible_vs_nonbible(df, save_path=save_path)

    # Save processed data
    if save_outputs:
        output_path = f"{output_dir}/analyzed_data.csv"
        df.to_csv(output_path, index=False)
        print(f"\n✓ Saved analyzed data to: {output_path}")

    print("\n" + "="*80)
    print("ANALYSIS COMPLETE!")
    print("="*80)

    return df


# ============================================================================
# QUICK ANALYSIS FUNCTIONS
# ============================================================================

def quick_summary(df: pd.DataFrame):
    """
    Print a quick summary of the translation dataset.

    Args:
        df: Input DataFrame
    """
    print("="*80)
    print("DATASET SUMMARY")
    print("="*80)

    print(f"\nShape: {df.shape}")
    print(f"Columns: {', '.join(df.columns.tolist())}")

    # Check for key columns
    has_metrics = 'BERTScore_F1' in df.columns
    has_bible = 'from_bible' in df.columns
    has_foreign = 'total_foreign_tokens' in df.columns

    print(f"\nMetrics available: {'Yes' if has_metrics else 'No'}")
    print(f"Bible classification: {'Yes' if has_bible else 'No'}")
    print(f"Foreign language tokens: {'Yes' if has_foreign else 'No'}")

    if has_metrics:
        print("\n" + "-"*80)
        print("METRIC STATISTICS")
        print("-"*80)

        metrics = ['BERTScore_F1', 'BLEU', 'COMET', 'WER']
        metrics = [m for m in metrics if m in df.columns]

        for metric in metrics:
            mean_val = df[metric].mean()
            std_val = df[metric].std()
            min_val = df[metric].min()
            max_val = df[metric].max()

            print(f"\n{metric}:")
            print(f"  Mean: {mean_val:.4f}")
            print(f"  Std:  {std_val:.4f}")
            print(f"  Min:  {min_val:.4f}")
            print(f"  Max:  {max_val:.4f}")


def compare_two_models(
    df1_path: str,
    df2_path: str,
    label1: str = "Model 1",
    label2: str = "Model 2",
    save_path: Optional[str] = None
):
    """
    Compare two translation models.

    Args:
        df1_path: Path to first CSV
        df2_path: Path to second CSV
        label1: Label for first model
        label2: Label for second model
        save_path: Path to save comparison plot
    """
    from coptic_translation_analysis import (
        preprocess_dataframe,
        compare_bertscore_distributions
    )

    print("="*80)
    print(f"COMPARING: {label1} vs {label2}")
    print("="*80)

    # Load and preprocess both datasets
    df1 = pd.read_csv(df1_path)
    df2 = pd.read_csv(df2_path)

    df1 = preprocess_dataframe(df1)
    df2 = preprocess_dataframe(df2)

    # Compare
    compare_bertscore_distributions(df1, df2, label1, label2, save_path)


# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    # Example 1: Analyze a single dataset
    print("\n" + "="*80)
    print("EXAMPLE 1: SINGLE DATASET ANALYSIS")
    print("="*80)

    # Replace with your actual file path
    df = analyze_translation_dataset(
        csv_path="gpt_results2.csv",
        calculate_metrics=False,  # Set True if metrics need to be calculated
        save_outputs=True,
        output_dir="./analysis_output"
    )

    # Quick summary
    quick_summary(df)

    # Example 2: Compare two models
    print("\n" + "="*80)
    print("EXAMPLE 2: MODEL COMPARISON")
    print("="*80)

    compare_two_models(
        df1_path="gpt_results_baseline.csv",
        df2_path="gpt_results2.csv",
        label1="Baseline",
        label2="With Lexicon+UD",
        save_path="./analysis_output/model_comparison.png"
    )

    # Example 3: Custom analysis
    print("\n" + "="*80)
    print("EXAMPLE 3: CUSTOM ANALYSIS")
    print("="*80)

    from coptic_translation_analysis import (
        preprocess_dataframe,
        extract_text_features,
        categorize_quality
    )

    # Load your data
    df = pd.read_csv("your_data.csv")

    # Custom preprocessing
    df = preprocess_dataframe(df)
    df = extract_text_features(df)
    df = categorize_quality(
        df,
        metric='BERTScore_F1',
        bins=[0, 0.75, 0.90, 1.0],
        labels=['Low', 'Medium', 'High']
    )

    # Your custom analysis here
    print("\nQuality distribution:")
    print(df['quality'].value_counts())

    # Filter high-quality translations
    high_quality = df[df['quality'] == 'High']
    print(f"\nHigh quality translations: {len(high_quality)}")
    print(f"Average word count: {high_quality['prediction_word_count'].mean():.2f}")


# ============================================================================
# ADDITIONAL UTILITY FUNCTIONS
# ============================================================================

def export_analysis_report(df: pd.DataFrame, output_path: str):
    """
    Export a comprehensive analysis report to text file.

    Args:
        df: Analyzed DataFrame
        output_path: Path to save the report
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("COPTIC TRANSLATION ANALYSIS REPORT\n")
        f.write("="*80 + "\n\n")

        # Dataset overview
        f.write("DATASET OVERVIEW\n")
        f.write("-"*80 + "\n")
        f.write(f"Total samples: {len(df)}\n")
        f.write(f"Columns: {len(df.columns)}\n\n")

        # Metric statistics
        if 'BERTScore_F1' in df.columns:
            f.write("METRIC STATISTICS\n")
            f.write("-"*80 + "\n")

            metrics = ['BERTScore_F1', 'BLEU', 'COMET', 'WER']
            metrics = [m for m in metrics if m in df.columns]

            for metric in metrics:
                f.write(f"\n{metric}:\n")
                f.write(f"  Mean: {df[metric].mean():.4f}\n")
                f.write(f"  Std:  {df[metric].std():.4f}\n")
                f.write(f"  Min:  {df[metric].min():.4f}\n")
                f.write(f"  Max:  {df[metric].max():.4f}\n")

        # Quality distribution
        if 'quality' in df.columns:
            f.write("\n" + "-"*80 + "\n")
            f.write("QUALITY DISTRIBUTION\n")
            f.write("-"*80 + "\n")
            for quality, count in df['quality'].value_counts().sort_index().items():
                percentage = (count / len(df)) * 100
                f.write(f"{quality}: {count} ({percentage:.1f}%)\n")

        # Best and worst examples
        if 'BERTScore_F1' in df.columns:
            f.write("\n" + "-"*80 + "\n")
            f.write("EXAMPLE TRANSLATIONS\n")
            f.write("-"*80 + "\n")

            # Best translation
            best_idx = df['BERTScore_F1'].idxmax()
            f.write(f"\nBest Translation (BERTScore F1: {df.loc[best_idx, 'BERTScore_F1']:.4f}):\n")
            f.write(f"Source: {df.loc[best_idx, 'source']}\n")
            f.write(f"Reference: {df.loc[best_idx, 'reference']}\n")
            f.write(f"Prediction: {df.loc[best_idx, 'predictions']}\n")

            # Worst translation
            worst_idx = df['BERTScore_F1'].idxmin()
            f.write(f"\nWorst Translation (BERTScore F1: {df.loc[worst_idx, 'BERTScore_F1']:.4f}):\n")
            f.write(f"Source: {df.loc[worst_idx, 'source']}\n")
            f.write(f"Reference: {df.loc[worst_idx, 'reference']}\n")
            f.write(f"Prediction: {df.loc[worst_idx, 'predictions']}\n")

    print(f"Report saved to: {output_path}")


def find_outliers(df: pd.DataFrame,
                 metric: str = 'BERTScore_F1',
                 percentile: float = 0.01) -> pd.DataFrame:
    """
    Find outlier translations (worst performers).

    Args:
        df: Input DataFrame
        metric: Metric to use for finding outliers
        percentile: Percentile threshold (0.01 = bottom 1%)

    Returns:
        DataFrame with outlier rows
    """
    threshold = df[metric].quantile(percentile)
    outliers = df[df[metric] <= threshold]

    print(f"Found {len(outliers)} outliers (bottom {percentile*100}%)")
    print(f"Threshold: {metric} <= {threshold:.4f}")

    return outliers


# Print helpful information
print("""
COPTIC TRANSLATION ANALYSIS TOOLKIT
===================================

Quick Start:
1. analyze_translation_dataset(csv_path, ...)  - Full analysis pipeline
2. quick_summary(df)                           - Quick dataset overview
3. compare_two_models(path1, path2, ...)       - Compare two models
4. export_analysis_report(df, path)            - Export text report
5. find_outliers(df, ...)                      - Find poor translations

For detailed documentation, see the main module.
""")